{"url": "https://api.github.com/repos/datalad/datalad/issues/1630", "repository_url": "https://api.github.com/repos/datalad/datalad", "labels_url": "https://api.github.com/repos/datalad/datalad/issues/1630/labels{/name}", "comments_url": "https://api.github.com/repos/datalad/datalad/issues/1630/comments", "events_url": "https://api.github.com/repos/datalad/datalad/issues/1630/events", "html_url": "https://github.com/datalad/datalad/pull/1630", "id": 240364586, "node_id": "MDExOlB1bGxSZXF1ZXN0MTI4NzYxNjc5", "number": 1630, "title": "Metadata facility update", "user": {"login": "mih", "id": 136479, "node_id": "MDQ6VXNlcjEzNjQ3OQ==", "avatar_url": "https://avatars.githubusercontent.com/u/136479?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mih", "html_url": "https://github.com/mih", "followers_url": "https://api.github.com/users/mih/followers", "following_url": "https://api.github.com/users/mih/following{/other_user}", "gists_url": "https://api.github.com/users/mih/gists{/gist_id}", "starred_url": "https://api.github.com/users/mih/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mih/subscriptions", "organizations_url": "https://api.github.com/users/mih/orgs", "repos_url": "https://api.github.com/users/mih/repos", "events_url": "https://api.github.com/users/mih/events{/privacy}", "received_events_url": "https://api.github.com/users/mih/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 25, "created_at": "2017-07-04T09:29:58Z", "updated_at": "2017-10-28T13:44:51Z", "closed_at": "2017-10-28T13:44:50Z", "author_association": "MEMBER", "active_lock_reason": null, "pull_request": {"url": "https://api.github.com/repos/datalad/datalad/pulls/1630", "html_url": "https://github.com/datalad/datalad/pull/1630", "diff_url": "https://github.com/datalad/datalad/pull/1630.diff", "patch_url": "https://github.com/datalad/datalad/pull/1630.patch"}, "body": "This is an attempt to adopt some ideas from huge PRs #1010, #1074, and #1077 on metadata that never made it into master. However, this time things will hopefully turn out a bit simpler....and they did.\r\n\r\nChanges\r\n\r\n- [x] unify JSON-LD context with a simple metadata key defintion list (related to #1601)\r\n- [x] `metadata --show-keys` (with human-readable description)\r\n- [x] `metadata` complains when setting undefined keys (`--permit-undefined-keys` is provided as an annoying bypass). This should get users to converge on the keys that we pre-define, as otherwise they are promted to define their own in datasets.\r\n- [x] get info on a datasets native metadata types\r\n- [x] go away from guessing metadata types. Either a dataset informs datalad that it conforms to a standard, or it doesn't. It is a maintainer's choice to advertise (some) native metadata. This does not affect any of the legacy metadata processing code (yet).\r\n- [x] be able to query metadata keys from native metadata (if it exists) and transparently amend custom datalad metadata with it -- while the custom one is preferred, in case alternative source have something to report. This is essentially doing metadata homogenization at the moment of per-dataset query -- without the needs for JSON-LD graphs. Obviously, this is limited to metadata understood be our parser, hence requires human creativity (but that is a good thing, and it only needs to happen once when a parser is written).\r\n- [x] support three meta data merge modes: init, add, reset (see dedicated test), same semantics as for metadata manipulation\r\n- [x] new-style metadata aggregation that minimized the number of moving parts across (sub)datasets -- no JSON-LD for aggregation anymore. Theoretical capability of an arbitrary number of metadata sources.\r\n- [x] metadata query transparently grabs aggregated metadata from the closest dataset if a path is unavailable -- yields identical, homogenized metadata results -- compared to a direct query.\r\n- [x] fast and powerful `search` reimplementation based on https://pypi.python.org/pypi/Whoosh (fucking amazing), on datalad.org we can even use the generated search index for \"search-as-you-type\" style operations...\r\n- [x] change agg object filename to something that avoid collision when the same dataset appears multiple times as a subdataset\r\n- [x] pray for enlightenment on how to adjust a broken test, where the mock test of the mocked results is mocking real hard, it hurts (poor michael has no clue)....\r\n- [x] fix #1793 -- do not silently ignore files in git or untracked ones\r\n- [x] native meta data parsers can now auto-obtain required annexed files\r\n- [x] make it possible to annex aggregated metadata objects. it is actually possible already, but it doesn't happen because of our preconfiguration\r\n- [x] implement `metadata` support for lookup of file-based metadata -- only few lines missing, `search` can already do it\r\n- [x] reevaluate the native metadata parser setup, now that the `metadata` is in place, it could be done in a leaner fashion -- presently the things feels bolted on -- this is related to the question what layer of code gets to decide whether files containing metadata are to be obtained #1796\r\n   - fix up aggregation of aggregated metadata, likely needs a gimme everything-you-know flag for `metadata (also see #1796)\r\n   - parsers should stop trying to get core metadata files that are not known to a dataset\r\n- [x] adjust search to benefit from the new behavior of `metadata` to already look up all relevant (and also aggregated) metadata. In particular, implement a recursive mode for `_query_aggregated_metadata()` to be used for generating the search index purely from aggregated metadata, and then run `metadata()` as before on the paths of all hits, to get the most up-to-date metadata report.\r\n- [x] put parentds field into search index documents to be able to quickly pull out all content for a specific dataset\r\n- [x] check what is wrong with CRCNS metadata extraction, shows unspecific error message\r\n```\r\n2017-09-28 11:35:30,130 [INFO   ] Aggregate metadata for dataset /home/mih/fullcopy/crcns/hc-2 \r\n2017-09-28 11:35:30,513 [ERROR  ] Failed to get dataset metadata (datacite): [Errno 2] No such file or directory: '/home/mih/fullcopy/crcns/hc-2/.datalad/meta.datacite.xml' [datacite.py:get_dataset_metadata:108] \r\n2017-09-28 11:35:30,634 [ERROR  ] Metadata extraction failed (see previous error message) [aggregate_metadata(/home/mih/fullcopy/crcns/hc-2)] \r\naggregate_metadata(error): /home/mih/fullcopy/crcns/hc-2 [Metadata extraction failed (see previous error message)]\r\n2017-09-28 11:40:45,172 [INFO   ] Aggregate metadata for dataset /home/mih/fullcopy/crcns/zipser-1 \r\n2017-09-28 11:40:45,387 [ERROR  ] Failed to get dataset metadata (datacite): [Errno 2] No such file or directory: '/home/mih/fullcopy/crcns/zipser-1/.datalad/meta.datacite.xml' [datacite.py:get_dataset_metadata:108] \r\n2017-09-28 11:40:45,515 [ERROR  ] Metadata extraction failed (see previous error message) [aggregate_metadata(/home/mih/fullcopy/crcns/zipser-1)] \r\naggregate_metadata(error): /home/mih/fullcopy/crcns/zipser-1 [Metadata extraction failed (see previous error message)]\r\n```\r\n- [x] aggregation of aggregated metadata seems to loose information (unpushed test available), needs fixing\r\n- [x] deal with metadata key conflict when multiple regexes match a filename (likely use existing merge_mode logic as when blending git-annex metadata) -- or we are just saying that conflicts are a sign of a stupid metadata representation anyways....\r\n- [x] aggregation must add info to dataset-global metadata on which keys (and optionally unique values) are available in content metadata\r\n- [x] RF search index creation into a 2-pass procedure: 1. fast(er) over just datasets in order to assess what keys have to be supported in the search index, and 2. (slower) overall all datasets and files to actually build the index. Rational: all keys need to be known before a complete schema can be constructed, and any dataset can define its own keys.\r\n- [x] rename `ontology_id` to `vocabulary_id`\r\n- [x] make it possible to actually search for content summary values. Info is already aggregated, but not yet accessible in the search index.\r\n- [x] EXIF metadata parser to turn datalad into a photo database\r\n- [x] Audio/ID3 metadata parser to turn datalad into a music database\r\n- [x] DICOM metadata parser that extracts metadata keys that are unique across all images in a series\r\n- now that we can pull out various flavors of metadata, we have to make things more flexible:\r\n  - [x] add generic config item to limit the fields of extracted metadata by size.  For example, EXIF includes thumbnails as metadata, ...\r\n  - [x] blacklist fields by name (regex, mayby)\r\n  - [x] aggregate content metadata, but have switch to not actuallt store it, but only use it to generate unique metadata key/value pairs across all files of a dataset\r\n  - [x] allow for turning off git-annex metadata extraction for aggregation. This can be crucial for datasets with tons of files to enable somewhat fast operation\r\n         this is now possible by adding `datalad.metadata.aggregate-content-datalad-core = no` to a dataset's config. Analog switched are tested for any activated parser, so one could selectively disable dataset-global or content-metadata for aggregation\r\n- [x] automatic invalidation of generated search indices, based on comparing the commit sha of aggregated metadata between now and generation time\r\n- [X] do fast local term resolution to give access to vocabulary definitions if any exist. Demo:\r\n```\r\n% datalad search --show-keys\r\n@id (unique identifier of an entity)\r\ncomment<albumartist> (http://purl.obolibrary.org/obo/NCIT_C25393)\r\ncomment<conductor> (http://purl.obolibrary.org/obo/NCIT_C25393)\r\ncomment<date> (http://purl.obolibrary.org/obo/NCIT_C25393)\r\ncomment<discnumber> (http://purl.obolibrary.org/obo/NCIT_C25393)\r\ncomment<encodedby> (http://purl.obolibrary.org/obo/NCIT_C25393)\r\ncomment<tracknumber> (http://purl.obolibrary.org/obo/NCIT_C25393)\r\ndcterms:rights (http://purl.org/dc/terms/rights)\r\nduration(s) {'unit (http://purl.obolibrary.org/obo/UO_0000000)': 'http://purl.obolibrary.org/obo/UO_0000010', '@id': 'https://www.w3.org/TR/owl-time/#Duration'}\r\nformat (http://purl.org/dc/elements/1.1/format)\r\nmusic:Composer (http://purl.org/ontology/mo/Composer)\r\nmusic:Genre (http://purl.org/ontology/mo/Genre)\r\nmusic:album (http://purl.org/ontology/mo/album)\r\nmusic:artist (http://purl.org/ontology/mo/artist)\r\nmusic:channels (http://purl.org/ontology/mo/channels)\r\nmusic:sample_rate (http://purl.org/ontology/mo/sample_rate)\r\nname (http://schema.org/name)\r\nparentds (path of the datasets that contains an entity)\r\npath (path name of an entity relative to the searched base dataset)\r\ntype {'@id': 'http://schema.org/type', 'description': 'type or category of a resource (e.g. file, dataset)'}\r\n```\r\n- [x] bring all tests back in alignment with new desired behavior\r\n- [x] adjust docs (but only after @yarikoptic gave his complete and unconditional blessing)\r\n- [x] extend `metadata` to report subdatasets for which metadata is present in a dataset\r\n- [x] decide whether to switch from JSON object storage to JSON streams for storage of aggregated metadata\r\n- [x] datalad needs to be aware which metadata version it can handle, and datasets need to report which versions of metadata they contain (@bpoldrack doing that via filename change)\r\n- [x] make it possible to store aggregated metadata in annexed files\r\n- [x] make it possible to store aggregated metadata in compressed binary files\r\n- [x] possibly depart from per-regex storage\r\n      - the only metadata format where this would be helpful is BIDS, but at the same time none of the BIDS tools supports reporting by filename regex, hence a complicated machinery would be necessary to reconstruct this\r\n      - alternatively, and possibly more productively we could use a standard compression algorithm\r\n\r\nCompression:\r\n\r\n```py\r\n# massively redundant metadata for 1M files\r\nd={k:k for k in range(10)}\r\ns=[dict(d, path='longfuckingfilename{:030}'.format(i)) for i in xrange(1000000)]\r\njson.dump(s, open('big.json', 'w'))\r\n```\r\n```\r\nmih@meiner /tmp % gzip -9 -c big.json > big.gz\r\nmih@meiner /tmp % xz -9 -c big.json > big.xz\r\n% ls -lh big*\r\n-rw-rw-r-- 1 mih mih 2,8M Okt 19 18:04 big.gz\r\n-rw-rw-r-- 1 mih mih 137M Okt 19 18:04 big.json\r\n-rw-rw-r-- 1 mih mih  83K Okt 19 18:04 big.xz\r\n```\r\n\r\nThere is no point in implementing a custom metadata reduction algorithm for BIDS.\r\n\r\nlzma (xz) support is built into Python 3.3+ and a backport is available for Python 2.\r\n\r\n- [x] fix issue of unsaved dataset old aggregate metadata file deletion when reaggregating with a metadata change #1912 \r\n- [x] pybids-based metadata parser doing a full report for all files, plus our custom enhancements from participants.tsv", "closed_by": {"login": "mih", "id": 136479, "node_id": "MDQ6VXNlcjEzNjQ3OQ==", "avatar_url": "https://avatars.githubusercontent.com/u/136479?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mih", "html_url": "https://github.com/mih", "followers_url": "https://api.github.com/users/mih/followers", "following_url": "https://api.github.com/users/mih/following{/other_user}", "gists_url": "https://api.github.com/users/mih/gists{/gist_id}", "starred_url": "https://api.github.com/users/mih/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mih/subscriptions", "organizations_url": "https://api.github.com/users/mih/orgs", "repos_url": "https://api.github.com/users/mih/repos", "events_url": "https://api.github.com/users/mih/events{/privacy}", "received_events_url": "https://api.github.com/users/mih/received_events", "type": "User", "site_admin": false}, "reactions": {"url": "https://api.github.com/repos/datalad/datalad/issues/1630/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/datalad/datalad/issues/1630/timeline", "performed_via_github_app": null}