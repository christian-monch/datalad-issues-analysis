{"url": "https://api.github.com/repos/datalad/datalad/issues/3876", "repository_url": "https://api.github.com/repos/datalad/datalad", "labels_url": "https://api.github.com/repos/datalad/datalad/issues/3876/labels{/name}", "comments_url": "https://api.github.com/repos/datalad/datalad/issues/3876/comments", "events_url": "https://api.github.com/repos/datalad/datalad/issues/3876/events", "html_url": "https://github.com/datalad/datalad/issues/3876", "id": 524088233, "node_id": "MDU6SXNzdWU1MjQwODgyMzM=", "number": 3876, "title": "Do download and addurl in one pass over the file content", "user": {"login": "yarikoptic", "id": 39889, "node_id": "MDQ6VXNlcjM5ODg5", "avatar_url": "https://avatars.githubusercontent.com/u/39889?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yarikoptic", "html_url": "https://github.com/yarikoptic", "followers_url": "https://api.github.com/users/yarikoptic/followers", "following_url": "https://api.github.com/users/yarikoptic/following{/other_user}", "gists_url": "https://api.github.com/users/yarikoptic/gists{/gist_id}", "starred_url": "https://api.github.com/users/yarikoptic/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yarikoptic/subscriptions", "organizations_url": "https://api.github.com/users/yarikoptic/orgs", "repos_url": "https://api.github.com/users/yarikoptic/repos", "events_url": "https://api.github.com/users/yarikoptic/events{/privacy}", "received_events_url": "https://api.github.com/users/yarikoptic/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-11-18T01:42:17Z", "updated_at": "2019-12-10T21:58:48Z", "closed_at": null, "author_association": "MEMBER", "active_lock_reason": null, "body": "When crawling a dataset with large files (>200GB per file) I noticed that the box (AWS EC2 instance) gets busy re-reading downloaded file by git-annex to compute its key. Ideally the key (checksum) gets computed in one pass along with downloading the data.\r\nAlthough a long shot, I think we might at some point consider some way to make adding new files avoid such two pass (download + `annex add`) over the file. I don't think the issue is crawler specific, so filing it in the datalad \"core\".", "closed_by": null, "reactions": {"url": "https://api.github.com/repos/datalad/datalad/issues/3876/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/datalad/datalad/issues/3876/timeline", "performed_via_github_app": null}