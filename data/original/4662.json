{"url": "https://api.github.com/repos/datalad/datalad/issues/4662", "repository_url": "https://api.github.com/repos/datalad/datalad", "labels_url": "https://api.github.com/repos/datalad/datalad/issues/4662/labels{/name}", "comments_url": "https://api.github.com/repos/datalad/datalad/issues/4662/comments", "events_url": "https://api.github.com/repos/datalad/datalad/issues/4662/events", "html_url": "https://github.com/datalad/datalad/issues/4662", "id": 645565812, "node_id": "MDU6SXNzdWU2NDU1NjU4MTI=", "number": 4662, "title": "Tentative setup for HTC computing via containers", "user": {"login": "mih", "id": 136479, "node_id": "MDQ6VXNlcjEzNjQ3OQ==", "avatar_url": "https://avatars.githubusercontent.com/u/136479?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mih", "html_url": "https://github.com/mih", "followers_url": "https://api.github.com/users/mih/followers", "following_url": "https://api.github.com/users/mih/following{/other_user}", "gists_url": "https://api.github.com/users/mih/gists{/gist_id}", "starred_url": "https://api.github.com/users/mih/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mih/subscriptions", "organizations_url": "https://api.github.com/users/mih/orgs", "repos_url": "https://api.github.com/users/mih/repos", "events_url": "https://api.github.com/users/mih/events{/privacy}", "received_events_url": "https://api.github.com/users/mih/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-06-25T13:32:03Z", "updated_at": "2021-07-23T07:02:03Z", "closed_at": "2021-07-23T07:02:03Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "This is not a issue. I am just using it to put a draft out here for people to find and comment.\r\n\r\nWhat is described here is a DataLad dataset holding and documenting the fmriprep processing of the eNKI dataset (or another big one). It utilizes several DataLad features (such as containerized execution, dataset nesting, provenance capture) in order to enabled FAIR-compliant results and computational reproducibility.\r\n\r\nThe whole dataset comprises five DataLad dataset. This superdataset and four subdatasets with the following purpose:\r\n\r\n- superdataset: code and documentation, versioned links to all subdatasets\r\ncode/pipelines: toolbox dataset, providing a preconfigured fmriprep equipped with a freesurfer license (not to be shared)\r\n- `sourcedata`: BIDS-formatted eNKI dataset\r\n- `fmriprep`: fmriprep output tree\r\n- `freesurfer`: dedicated Freesurfer output tree produced by fmriprep\r\n\r\nAll code is located in the `code/` directory. There are two essential pieces that make up an HTCondor-based parallelized fmriprep of eNKI.\r\n\r\n- `code/fmriprep_all_participants.submit` is an HTCondor submit file that submits on job per participant to HTCondor. Here the resource requirements of individual jobs are specified, and the execution environment for each and all jobs is defined.\r\n- `code/fmriprep_participant_job`\r\n\r\nIn order for the submission to work correctly, the `sourcedata/` subdataset needs to be installed. Its top-level subdirectories are used to discover the participant IDs (run `datalad get -n sourcedata` in the root of the superdataset).\r\n\r\nLog files for jobs will be placed outside the superdataset at `../logs`\r\n\r\nThe outputs for each job will be recorded in a dedicated branch in each of the output subdatasets. Branch names will be `condor-<cluster>-<process>`, where `cluster` and `process` are the HTCcondor job identifiers, and enable matching to the generated log files, in case something went wrong. All output branch have to be merged in a dedicated subsequent step, after computations are completed.\r\n\r\nThe submit file:\r\n\r\n```\r\nuniverse       = vanilla\r\n# this is currently necessary, because otherwise the\r\n# bundles git in git-annex-standalone breaks\r\n# but it should be removed eventually\r\nget_env        = True\r\n# resource requirements for each job\r\nrequest_cpus   = 1\r\nrequest_memory = 8G\r\nrequest_disk   = 90G\r\nexecutable     = $ENV(PWD)/code/fmriprep_participant_job\r\n# the job expects to environment variables for labeling and synchronization\r\nenvironment = \"CONDOR_JOBID=$(Cluster).$(Process) DSLOCKFILE=$ENV(PWD)/.git/datalad_lock\"\r\nlog    = $ENV(PWD)/../logs/$(Cluster).$(Process).log\r\noutput = $ENV(PWD)/../logs/$(Cluster).$(Process).out\r\nerror  = $ENV(PWD)/../logs/$(Cluster).$(Process).err\r\narguments = $(subid)\r\n# find all participants, based on the subdirectory names in the source dataset\r\n# each relative path to such a subdirectory with become the value of `subid`\r\n# and another job is queued. Will queue a total number of jobs matching the\r\n# number of matching subdirectories\r\nqueue subid matching dirs sourcedata/sub-*\r\n```\r\n\r\nThe job \"payload\"\r\n\r\n```sh\r\n#!/bin/bash\r\n# fail whenever something is fishy, use -x to get verbose logfiles\r\nset -e -u -x\r\n# we pass in \"sourcedata/sub-...\", extract subject id from it\r\nsubid=$(basename $1)\r\n# this is all running under /tmp inside a condor job, /tmp is a performant\r\n# local filesystem\r\ncd /tmp\r\n# get the output dataset, which inclues the inputs as well\r\n# flock makes sure that this does not interfere with another job\r\n# finishing at the same time, and pushing its results back\r\nflock --verbose $DSLOCKFILE \\\r\n\tdatalad clone --reckless ephemeral URL_OF_ROOT_DATASET ds\r\n# all following actions are performed in the context of the superdataset\r\ncd ds\r\n# obtain dataset with fmriprep singularity container and pre-configured\r\n# pipeline call; also get the output dataset to prep them for output\r\n# consumption, we need to tune them for this particular job\r\ndatalad get -n code/pipelines fmriprep freesurfer\r\n# checkout new branches in both subdatasets\r\n# this enables us to store the results of this job, and push them back\r\n# without interference from other jobs\r\ngit -C fmriprep checkout -b \"condor-$CONDOR_JOBID\"\r\ngit -C freesurfer checkout -b \"condor-$CONDOR_JOBID\"\r\n# create workdir for fmriprep inside to simplify singularity call\r\n# PWD will be available in the container\r\nmkdir -p .git/tmp/wdir\r\n# pybids (inside fmriprep) gets angry when it sees dangling symlinks\r\n# of .json files -- wipe them out, spare only those that belong to\r\n# the participant we want to process in this job\r\nfind sourcedata -mindepth 2 -name '*.json' -a ! -wholename '*'/\"$1\"/'*' -delete\r\n# the meat of the matter, add actual parameterization after --participant-label\r\ndatalad containers-run \\\r\n  --explicit \\\r\n  -o freesurfer -o fmriprep \\\r\n  -i \"$1\" \\\r\n  -n code/pipelines/fmriprep \\\r\n  sourcedata . participant \\\r\n  --n_cpus 1 \\\r\n  --skull-strip-fixed-seed \\\r\n  -w .git/tmp/wdir \\\r\n  --skip-bids-validation \\\r\n  --participant-label \"$subid\"\r\n# selectively push outputs only\r\n# ignore root dataset, despite recorded changes, needs coordinated\r\n# merge at receiving end\r\nflock --verbose $DSLOCKFILE datalad push -d fmriprep --\r\n```", "closed_by": {"login": "adswa", "id": 29738718, "node_id": "MDQ6VXNlcjI5NzM4NzE4", "avatar_url": "https://avatars.githubusercontent.com/u/29738718?v=4", "gravatar_id": "", "url": "https://api.github.com/users/adswa", "html_url": "https://github.com/adswa", "followers_url": "https://api.github.com/users/adswa/followers", "following_url": "https://api.github.com/users/adswa/following{/other_user}", "gists_url": "https://api.github.com/users/adswa/gists{/gist_id}", "starred_url": "https://api.github.com/users/adswa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/adswa/subscriptions", "organizations_url": "https://api.github.com/users/adswa/orgs", "repos_url": "https://api.github.com/users/adswa/repos", "events_url": "https://api.github.com/users/adswa/events{/privacy}", "received_events_url": "https://api.github.com/users/adswa/received_events", "type": "User", "site_admin": false}, "reactions": {"url": "https://api.github.com/repos/datalad/datalad/issues/4662/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/datalad/datalad/issues/4662/timeline", "performed_via_github_app": null}