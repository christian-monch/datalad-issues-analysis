{"url": "https://api.github.com/repos/datalad/datalad/issues/5022", "repository_url": "https://api.github.com/repos/datalad/datalad", "labels_url": "https://api.github.com/repos/datalad/datalad/issues/5022/labels{/name}", "comments_url": "https://api.github.com/repos/datalad/datalad/issues/5022/comments", "events_url": "https://api.github.com/repos/datalad/datalad/issues/5022/events", "html_url": "https://github.com/datalad/datalad/pull/5022", "id": 719780003, "node_id": "MDExOlB1bGxSZXF1ZXN0NTAxOTE0MzQ5", "number": 5022, "title": "NF+OPT+ENH: a helper to parallelize operations (via threads)", "user": {"login": "yarikoptic", "id": 39889, "node_id": "MDQ6VXNlcjM5ODg5", "avatar_url": "https://avatars.githubusercontent.com/u/39889?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yarikoptic", "html_url": "https://github.com/yarikoptic", "followers_url": "https://api.github.com/users/yarikoptic/followers", "following_url": "https://api.github.com/users/yarikoptic/following{/other_user}", "gists_url": "https://api.github.com/users/yarikoptic/gists{/gist_id}", "starred_url": "https://api.github.com/users/yarikoptic/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yarikoptic/subscriptions", "organizations_url": "https://api.github.com/users/yarikoptic/orgs", "repos_url": "https://api.github.com/users/yarikoptic/repos", "events_url": "https://api.github.com/users/yarikoptic/events{/privacy}", "received_events_url": "https://api.github.com/users/yarikoptic/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 63778592, "node_id": "MDU6TGFiZWw2Mzc3ODU5Mg==", "url": "https://api.github.com/repos/datalad/datalad/labels/enhancement", "name": "enhancement", "color": "0052cc", "default": true, "description": ""}, {"id": 289240596, "node_id": "MDU6TGFiZWwyODkyNDA1OTY=", "url": "https://api.github.com/repos/datalad/datalad/labels/performance", "name": "performance", "color": "f4b2d8", "default": false, "description": "Improve performance of an existing feature"}, {"id": 379953235, "node_id": "MDU6TGFiZWwzNzk5NTMyMzU=", "url": "https://api.github.com/repos/datalad/datalad/labels/do-in-parallel", "name": "do-in-parallel", "color": "ffda63", "default": false, "description": ""}, {"id": 390153891, "node_id": "MDU6TGFiZWwzOTAxNTM4OTE=", "url": "https://api.github.com/repos/datalad/datalad/labels/UX", "name": "UX", "color": "0052cc", "default": false, "description": "user experience"}, {"id": 1746781627, "node_id": "MDU6TGFiZWwxNzQ2NzgxNjI3", "url": "https://api.github.com/repos/datalad/datalad/labels/merge-if-ok", "name": "merge-if-ok", "color": "fbca04", "default": false, "description": "OP considers this work done, and requests review/merge"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 21, "created_at": "2020-10-13T01:27:34Z", "updated_at": "2020-11-02T14:28:05Z", "closed_at": "2020-10-30T20:58:16Z", "author_association": "MEMBER", "active_lock_reason": null, "pull_request": {"url": "https://api.github.com/repos/datalad/datalad/pulls/5022", "html_url": "https://github.com/datalad/datalad/pull/5022", "diff_url": "https://github.com/datalad/datalad/pull/5022.diff", "patch_url": "https://github.com/datalad/datalad/pull/5022.patch"}, "body": "We virtually never see datalad \"busy\".  I believe that even with the simplest parallelization via threads, thus simpler mechanisms to orchestrate, we can make datalad more efficient by working with multiple git/git-annex processes at once, e.g in cases of multiple datasets to work on\r\n\r\n- the primary \"motivational\" use case is `addurls` which might need to create a large number of subdatasets.  IMHO the most logical (single) point of \"optimization\" would be the `create` command if it was made to accept multiple datasets (#3233), but since it is not the case, the last commit (ATM ca87b8ad292e17106918a76cd66e87ae9f34d299) does it within `addurls`\r\n  - unfortunately it is not 100% working yet due to a freshly discovered #5021 (but works on a simpler example without nesting within added unittest for ConsumerProducer)\r\n- also includes 39b03e7818074c47a38f08751ed2feaf19570509 RF/ENH which is IMHO generally useful, and if agreed, I can submit it in a separate PR.  It removes log filtering and makes `ProgressHandler` redirect to the default handler for non-progress log messages AFTER cleaning up all progress bars, and then refreshing them after that.  That allows to avoid interference between regular log and progress logs bars.  Even in DEBUG heavy output, progress bar is nicely lingering at the bottom\r\n   - the only possible intererences with progressbars which would remain AFAIK are 1). direct unhandled output to stdout/stderr from `run` processes; ~~2). results records.  For 2), I think if we could add a \"log handler\" which would not format them at all, and then still use ProgressHandler logger - we could have those records nicely displayed without interfering with progress bars even when some processes are still ongoing.~~ result records already go through `ui.message` which stops all progress bars first and then shows again (I just submitted a minor #5040 which unlikely relates).  There is no thread safety measures there, so I think it *might* (have not analyzed) interfere with clear/refresh progress bars done by `ProgressHandler` (is logging thread safe?)\r\n\r\n- an aspect I wanted to retain is to keep generators generators, be it a producer (e.g. listing of subdatasets) or consumer (some operation on those subdatasets), while also providing progress bars. With this implementation, we will get progress report as soon as producer starts producing entries we process by a consumer, ~~and only whenever producer exhausts itself, and thus we know total number -- we get a proper progressbar~~ as of 0ce386ea0c7ee12537e95e14c83ce7d07906efa0 progress bar appears right away but total might grow as well as we get more stuff to do.\r\n\r\n<details>\r\n<summary>demo 1: simple</summary> \r\n\r\nHere is a quick demo from a unittest (with tune up to make it run longer) with a \"slow arange\" output from which gets consumed by even slower consumer ;)  It shows that we start without progressbar and then it appears.  2nd run is done with `DATALAD_LOG_LEVEL=DEBUG` to show how progressbar lingers at the bottom ;)\r\n\r\n[![asciicast](https://asciinema.org/a/T0nIxwDspRopN0n5rlOBCR04A.svg)](https://asciinema.org/a/T0nIxwDspRopN0n5rlOBCR04A)\r\n\r\nNB just now mentioned that some empty lines in the output start breeding -- I think it is some side-effect from progressbars (not logs), which I see from time to time in other cases -- that 'clean' doesn't fully clean?\r\n\r\nAnd while testing that test_creatsubdatasets (just beef it up to more datasets) you finally can see datalad busy doing something useful ;)\r\n\r\n</details>\r\n\r\n <details>\r\n<summary>demo 2: install </summary> \r\n\r\n[![asciicast](https://asciinema.org/a/mNpwoYFt66IwO7x2UofC6QVyf.svg)](https://asciinema.org/a/mNpwoYFt66IwO7x2UofC6QVyf)\r\n</details>\r\n\r\nTODOs\r\n- ~~gain feedback on either it is the right direction to pursue ;)~~ seems it hasn't excited anyone so plowing forward as-envisioned\r\n- [-] better names? (e.g. for `ProducerConsumer ` etc)\r\n- decide on how to \"split\" jobs specification between what to pass to annex and what to be used by datalad.\r\n  - [x] I think we just need to add additional `datalad.jobs` common config variable which would be consulted if args/cmdline did not provide any (i.e stayed `None`).  `-J` in cmdline will probably just stay as the one for both annex and datalad\r\n-  initial adoption/testing\r\n   - [x] `addurls` - parallel `create`  . Closes #4990 (blocked by #5021). #5049 resolves it to addurls-sufficient degree. Merged in this PR for \"completness\"\r\n   - [x] `install`/`get` - for already \"registered\" subdatasets installation (see: #4182;  #450 - TODO) \r\n   - [x] `save` (#2880)\r\n- [x] docstrings and examples\r\n- [x] possibly remove iterators.py with `IteratorWithAggregation` (from dandi) which I first thought to reuse but then ended up just melding its code with parallelization + log_progress\r\n- [x] should all `log_progress` logic be moved \"outside\" (and helper like `ProducerConsumerLogged` be provided)? (could probably just consume records from the ProducerConsumer, the only tricky point is to pass \"total\" into it, but could be done by being the only place to use `lgr` for)  That would allow to use it for use cases where returned values are not our structured records\r\n- [?] \"properly\" handle \"on_failure='error'\".  I do feel that I have observed it not interrupting properly, but it might had been due to outside higher level loop we have in gathering result records...\r\n\r\nCrazy ideas (for future):\r\n- auto scaling: ATM we pre-define number of jobs... But in the code I already added provision for delaying submission of a new job (e.g. waiting for future creating a parent dataset to be done first). So we could use that to establish an upper-limit (e.g. 2*number of cores) and then feed it with new \"futures\" as long as we see that current PID CPU utilization is still far from 100%.  Alternatively, since intended for quite consistent tasks, we could time how long each future took (while operating with minimal specified parallelization), and then provide balancing based on average time for a process to complete not increasing significantly (over-committing IO or CPU).  May be there is already known libraries/algorithms to do that.\r\n\r\nNot here:\r\n- allow for parallel `get/install` over ssh with prompt.  May be https://github.com/datalad/datalad/issues/5034 ?\r\n- pretty much any command which operates recursively across datasets could adopt this approach (`remove`, `drop`) and even `subdatasets` itself and other core commands like `diff` and `status` !\r\n- some late commits are BF which might be applicable to `maint` as well", "closed_by": {"login": "yarikoptic", "id": 39889, "node_id": "MDQ6VXNlcjM5ODg5", "avatar_url": "https://avatars.githubusercontent.com/u/39889?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yarikoptic", "html_url": "https://github.com/yarikoptic", "followers_url": "https://api.github.com/users/yarikoptic/followers", "following_url": "https://api.github.com/users/yarikoptic/following{/other_user}", "gists_url": "https://api.github.com/users/yarikoptic/gists{/gist_id}", "starred_url": "https://api.github.com/users/yarikoptic/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yarikoptic/subscriptions", "organizations_url": "https://api.github.com/users/yarikoptic/orgs", "repos_url": "https://api.github.com/users/yarikoptic/repos", "events_url": "https://api.github.com/users/yarikoptic/events{/privacy}", "received_events_url": "https://api.github.com/users/yarikoptic/received_events", "type": "User", "site_admin": false}, "reactions": {"url": "https://api.github.com/repos/datalad/datalad/issues/5022/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/datalad/datalad/issues/5022/timeline", "performed_via_github_app": null}