{"url": "https://api.github.com/repos/datalad/datalad/issues/5890", "repository_url": "https://api.github.com/repos/datalad/datalad", "labels_url": "https://api.github.com/repos/datalad/datalad/issues/5890/labels{/name}", "comments_url": "https://api.github.com/repos/datalad/datalad/issues/5890/comments", "events_url": "https://api.github.com/repos/datalad/datalad/issues/5890/events", "html_url": "https://github.com/datalad/datalad/issues/5890", "id": 970224631, "node_id": "MDU6SXNzdWU5NzAyMjQ2MzE=", "number": 5890, "title": "Upload of large files into S3 bucket fails - S3Error {s3StatusCode = Status {statusCode = 400, statusMessage = \"Bad Request\"}, s3ErrorCode = \"EntityTooLarge\", s3ErrorMessage = \"Your proposed upload exceeds the maximum allowed object size.\"", "user": {"login": "m-petersen", "id": 39294105, "node_id": "MDQ6VXNlcjM5Mjk0MTA1", "avatar_url": "https://avatars.githubusercontent.com/u/39294105?v=4", "gravatar_id": "", "url": "https://api.github.com/users/m-petersen", "html_url": "https://github.com/m-petersen", "followers_url": "https://api.github.com/users/m-petersen/followers", "following_url": "https://api.github.com/users/m-petersen/following{/other_user}", "gists_url": "https://api.github.com/users/m-petersen/gists{/gist_id}", "starred_url": "https://api.github.com/users/m-petersen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/m-petersen/subscriptions", "organizations_url": "https://api.github.com/users/m-petersen/orgs", "repos_url": "https://api.github.com/users/m-petersen/repos", "events_url": "https://api.github.com/users/m-petersen/events{/privacy}", "received_events_url": "https://api.github.com/users/m-petersen/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2021-08-13T09:22:43Z", "updated_at": "2021-08-25T10:54:11Z", "closed_at": "2021-08-25T10:54:11Z", "author_association": "NONE", "active_lock_reason": null, "body": "#### What is the problem?\r\nI aim for establishing a S3 special remote for our local S3 Bucket in a dataset of singularity containers to make that dataset shareable across clients. When datalad tries to upload container images >5gb it fails with \r\n\r\n>[ERROR  ]   S3Error {s3StatusCode = Status {statusCode = 400, statusMessage = \"Bad Request\"}, s3ErrorCode = \"EntityTooLarge\", s3ErrorMessage = \"Your proposed upload exceeds the maximum allowed object size.\", s3ErrorResource = Nothing, s3ErrorHostId = Just \"66YMxRsk-CPKLdiois3r7H3pQ_ewu2bDN_5CwpuTh9k\", s3ErrorAccessKeyId = Nothing, s3ErrorStringToSign = Nothing, s3ErrorBucket = Nothing, s3ErrorEndpointRaw = Nothing, s3ErrorEndpoint = Nothing} \r\n\r\nI contacted the sysadmin for our S3 storage and he said that the Bucket is configured to allow files >5gb. His take on this is that the S3 client used by datalad under the hood won't switch upload \"strategies\" as for example aws cli would do (PUT for objects <5gb and MPU for >5gb).\r\n\r\n<details>\r\n<summary>The full verbose output is</summary> \r\n\r\n```\r\n$ datalad -l debug push --to github\r\n\r\n[DEBUG  ] Command line args 1st pass for DataLad 0.14.4. Parsed: Namespace() Unparsed: ['push', '--to', 'github']\r\n[DEBUG  ] Discovering plugins\r\n[DEBUG  ] Building doc for <class 'datalad.core.local.status.Status'>\r\n[DEBUG  ] Building doc for <class 'datalad.core.local.diff.Diff'>\r\n[DEBUG  ] Building doc for <class 'datalad.core.distributed.push.Push'>\r\n[DEBUG  ] Parsing known args among ['/work/fatx405/miniconda3/bin/datalad', '-l', 'debug', 'push', '--to', 'github']\r\n[DEBUG  ] Async run:\r\n|  cwd=None\r\n|  cmd=['git', '--git-dir=', 'config', '-z', '-l', '--show-origin']\r\n[DEBUG  ] Launching process ['git', '--git-dir=', 'config', '-z', '-l', '--show-origin']\r\n[DEBUG  ] Process 28681 started\r\n[DEBUG  ] Waiting for process 28681 to complete\r\n[DEBUG  ] Process 28681 exited with return code 0\r\n[DEBUG  ] Determined class of decorated function: <class 'datalad.core.distributed.push.Push'>\r\n[DEBUG  ] Resolved dataset for pushing: /work/fatx405/projects/envs\r\n[DEBUG  ] Async run:\r\n|  cwd=/work/fatx405/projects/envs\r\n|  cmd=['git', 'config', '-z', '-l', '--show-origin']\r\n[DEBUG  ] Launching process ['git', 'config', '-z', '-l', '--show-origin']\r\n[DEBUG  ] Process 28715 started\r\n[DEBUG  ] Waiting for process 28715 to complete\r\n[DEBUG  ] Process 28715 exited with return code 0\r\n[DEBUG  ] Async run:\r\n|  cwd=/work/fatx405/projects/envs\r\n|  cmd=['git', 'config', '-z', '-l', '--show-origin', '--file', '/work/fatx405/projects/envs/.datalad/config']\r\n[DEBUG  ] Launching process ['git', 'config', '-z', '-l', '--show-origin', '--file', '/work/fatx405/projects/envs/.datalad/config']\r\n[DEBUG  ] Process 28765 started\r\n[DEBUG  ] Waiting for process 28765 to complete\r\n[DEBUG  ] Process 28765 exited with return code 0\r\n[DEBUG  ] Resolved dataset for difference reporting: /work/fatx405/projects/envs\r\n[DEBUG  ] Diff Dataset(/work/fatx405/projects/envs) from 'None' to 'HEAD'\r\n[DEBUG  ] AnnexRepo(/work/fatx405/projects/envs).get_content_info(...)\r\n[DEBUG  ] Query repo: ['ls-tree', 'HEAD', '-z', '-r', '--full-tree', '-l']\r\n[DEBUG  ] Async run:\r\n|  cwd=/work/fatx405/projects/envs\r\n|  cmd=['git', '-c', 'diff.ignoreSubmodules=none', 'ls-tree', 'HEAD', '-z', '-r', '--full-tree', '-l']\r\n[DEBUG  ] Launching process ['git', '-c', 'diff.ignoreSubmodules=none', 'ls-tree', 'HEAD', '-z', '-r', '--full-tree', '-l']\r\n[DEBUG  ] Process 28810 started\r\n[DEBUG  ] Waiting for process 28810 to complete\r\n[DEBUG  ] Process 28810 exited with return code 0\r\n[DEBUG  ] Done query repo: ['ls-tree', 'HEAD', '-z', '-r', '--full-tree', '-l']\r\n[DEBUG  ] Done AnnexRepo(/work/fatx405/projects/envs).get_content_info(...)\r\n[DEBUG  ] Attempt push of Dataset at /work/fatx405/projects/envs\r\n[DEBUG  ] Discovered publication dependencies for 'github': ['s3']'\r\n[DEBUG  ] Async run:                                \r\n|  cwd=/work/fatx405/projects/envs\r\n|  cmd=['git', '-c', 'diff.ignoreSubmodules=none', 'push', '--progress', '--porcelain', '--dry-run', 'github']\r\n[DEBUG  ] Launching process ['git', '-c', 'diff.ignoreSubmodules=none', 'push', '--progress', '--porcelain', '--dry-run', 'github']\r\n[DEBUG  ] Process 28917 started                    \r\n[DEBUG  ] Waiting for process 28917 to complete    \r\n[DEBUG  ] Non-progress stderr: b'fatal: The current branch main has no upstream branch.\\n'\r\n[DEBUG  ] Non-progress stderr: b'To push the current branch and set the remote as upstream, use\\n'\r\n[DEBUG  ] Non-progress stderr: b'\\n'                \r\n[DEBUG  ] Non-progress stderr: b'    git push --set-upstream github main\\n'\r\n[DEBUG  ] Non-progress stderr: b'\\n'                \r\n[DEBUG  ] Process 28917 exited with return code 128\r\n[DEBUG  ] Dry-run push to check push configuration failed, assume no configuration: CommandError: 'git -c diff.ignoreSubmodules=none push --progress --porcelain --dry-run github' failed with exitcode 128 under /work/fatx405/projects/envs [err: 'fatal: The current branch main has no upstream branch.\r\nTo push the current branch and set the remote as upstream, use\r\n\r\n    git push --set-upstream github main']\r\n[DEBUG  ] Async run:                                \r\n|  cwd=/work/fatx405/projects/envs\r\n|  cmd=['git', '-c', 'diff.ignoreSubmodules=none', 'symbolic-ref', 'HEAD']\r\n[DEBUG  ] Launching process ['git', '-c', 'diff.ignoreSubmodules=none', 'symbolic-ref', 'HEAD']\r\n[DEBUG  ] Process 28948 started                    \r\n[DEBUG  ] Waiting for process 28948 to complete    \r\n[DEBUG  ] Process 28948 exited with return code 0  \r\n[DEBUG  ] Async run:                                \r\n|  cwd=/work/fatx405/projects/envs\r\n|  cmd=['git', '-c', 'diff.ignoreSubmodules=none', 'symbolic-ref', 'HEAD']\r\n[DEBUG  ] Launching process ['git', '-c', 'diff.ignoreSubmodules=none', 'symbolic-ref', 'HEAD']\r\n[DEBUG  ] Process 28980 started                    \r\n[DEBUG  ] Waiting for process 28980 to complete    \r\n[DEBUG  ] Process 28980 exited with return code 0  \r\n[DEBUG  ] No sync necessary, no corresponding branch detected\r\n[DEBUG  ] Async run:                                \r\n|  cwd=/work/fatx405/projects/envs                  \r\n|  cmd=['git', '-c', 'diff.ignoreSubmodules=none', 'push', '--progress', '--porcelain', '--dry-run', 's3']\r\n[DEBUG  ] Launching process ['git', '-c', 'diff.ignoreSubmodules=none', 'push', '--progress', '--porcelain', '--dry-run', 's3']\r\n[DEBUG  ] Process 29011 started                    \r\n[DEBUG  ] Waiting for process 29011 to complete    \r\n[DEBUG  ] Non-progress stderr: b'fatal: The current branch main has no upstream branch.\\n'              \r\n[DEBUG  ] Non-progress stderr: b'To push the current branch and set the remote as upstream, use\\n'      \r\n[DEBUG  ] Non-progress stderr: b'\\n'                \r\n[DEBUG  ] Non-progress stderr: b'    git push --set-upstream s3 main\\n'                                \r\n[DEBUG  ] Non-progress stderr: b'\\n'                \r\n[DEBUG  ] Process 29011 exited with return code 128\r\n[DEBUG  ] Dry-run push to check push configuration failed, assume no configuration: CommandError: 'git -c diff.ignoreSubmodules=none push --progress --porcelain --dry-run s3' failed with exitcode 128 under /work/fatx405/projects/envs [err: 'fatal: The current branch main has no upstream branch.\r\nTo push the current branch and set the remote as upstream, use\r\n\r\n    git push --set-upstream s3 main']\r\n[DEBUG  ] Async run:                                \r\n|  cwd=/work/fatx405/projects/envs                  \r\n|  cmd=['git', '-c', 'diff.ignoreSubmodules=none', 'symbolic-ref', 'HEAD']\r\n[DEBUG  ] Launching process ['git', '-c', 'diff.ignoreSubmodules=none', 'symbolic-ref', 'HEAD']        \r\n[DEBUG  ] Process 29042 started                    \r\n[DEBUG  ] Waiting for process 29042 to complete    \r\n[DEBUG  ] Process 29042 exited with return code 0  \r\n[DEBUG  ] Async run:                                \r\n|  cwd=/work/fatx405/projects/envs                  \r\n|  cmd=['git', '-c', 'diff.ignoreSubmodules=none', 'symbolic-ref', 'HEAD']\r\n[DEBUG  ] Launching process ['git', '-c', 'diff.ignoreSubmodules=none', 'symbolic-ref', 'HEAD']        \r\n[DEBUG  ] Process 29073 started                    \r\n[DEBUG  ] Waiting for process 29073 to complete    \r\n[DEBUG  ] Process 29073 exited with return code 0  \r\n[DEBUG  ] No sync necessary, no corresponding branch detected                                          \r\n[DEBUG  ] Launching process ['/work/fatx405/miniconda3/bin/python', '--version']                        \r\n[DEBUG  ] Process 29122 started                    \r\n[DEBUG  ] Waiting for process 29122 to complete    \r\n[DEBUG  ] Process 29122 exited with return code 0  \r\n[DEBUG  ] Async run:                                \r\n|  cwd=None                                        \r\n|  cmd=['git', 'annex', 'version', '--raw']\r\n[DEBUG  ] Launching process ['git', 'annex', 'version', '--raw']                                        \r\n[DEBUG  ] Process 29123 started                    \r\n[DEBUG  ] Waiting for process 29123 to complete    \r\n[DEBUG  ] Process 29123 exited with return code 0  \r\n[DEBUG  ] Async run:                                \r\n|  cwd=/work/fatx405/projects/envs                  \r\n|  cmd=['git', '-c', 'diff.ignoreSubmodules=none', 'annex', 'findref', '--copies', '0', 'HEAD', '--json', '--json-error-messages', '-c', 'annex.dotfiles=true', '-c', 'annex.retry=3']\r\n[DEBUG  ] Launching process ['git', '-c', 'diff.ignoreSubmodules=none', 'annex', 'findref', '--copies', '0', 'HEAD', '--json', '--json-error-messages', '-c', 'annex.dotfiles=true', '-c', 'annex.retry=3']\r\n[DEBUG  ] Process 29173 started                    \r\n[DEBUG  ] Waiting for process 29173 to complete    \r\n[DEBUG  ] Process 29173 exited with return code 0  \r\n[DEBUG  ] Async run:                                \r\n|  cwd=/work/fatx405/projects/envs                  \r\n|  cmd=['git', '-c', 'diff.ignoreSubmodules=none', 'annex', 'wanted', 's3', '-c', 'annex.dotfiles=true', '-c', 'annex.retry=3']\r\n[DEBUG  ] Launching process ['git', '-c', 'diff.ignoreSubmodules=none', 'annex', 'wanted', 's3', '-c', 'annex.dotfiles=true', '-c', 'annex.retry=3']\r\n[DEBUG  ] Process 29301 started                    \r\n[DEBUG  ] Waiting for process 29301 to complete    \r\n[DEBUG  ] Process 29301 exited with return code 0  \r\n[DEBUG  ] Push data from Dataset(/work/fatx405/projects/envs) to 's3'                                  \r\n[DEBUG  ] Counted 68394561599 bytes of annex data to transfer                                          \r\n[DEBUG  ] Async run:                                \r\n|  cwd=/work/fatx405/projects/envs                  \r\n|  cmd=['git', '-c', 'diff.ignoreSubmodules=none', 'annex', 'copy', '--batch', '-z', '--to', 's3', '--fast', '--json', '--json-error-messages', '--json-progress', '-c', 'annex.dotfiles=true', '-c', 'annex.retry=3']\r\n[DEBUG  ] Launching process ['git', '-c', 'diff.ignoreSubmodules=none', 'annex', 'copy', '--batch', '-z', '--to', 's3', '--fast', '--json', '--json-error-messages', '--json-progress', '-c', 'annex.dotfiles=true', '-c', 'annex.retry=3']\r\n[DEBUG  ] Process 29381 started                    \r\n[DEBUG  ] Waiting for process 29381 to complete    \r\n[ERROR  ]   S3Error {s3StatusCode = Status {statusCode = 400, statusMessage = \"Bad Request\"}, s3ErrorCode = \"EntityTooLarge\", s3ErrorMessage = \"Your proposed upload exceeds the maximum allowed object size.\", s3ErrorResource = Nothing, s3ErrorHostId = Just \"66YMxRsk-CPKLdiois3r7H3pQ_ewu2bDN_5CwpuTh9k\", s3ErrorAccessKeyId = Nothing, s3ErrorStringToSign = Nothing, s3ErrorBucket = Nothing, s3ErrorEndpointRaw = Nothing, s3ErrorEndpoint = Nothing}\r\n[ERROR  ]   S3Error {s3StatusCode = Status {statusCode = 400, statusMessage = \"Bad Request\"}, s3ErrorCode = \"EntityTooLarge\", s3ErrorMessage = \"Your proposed upload exceeds the maximum allowed object size.\", s3ErrorResource = Nothing, s3ErrorHostId = Just \"66YMxRsk-CPKLdiois3r7H3pQ_ewu2bDN_5CwpuTh9k\", s3ErrorAccessKeyId = Nothing, s3ErrorStringToSign = Nothing, s3ErrorBucket = Nothing, s3ErrorEndpointRaw = Nothing, s3ErrorEndpoint = Nothing}\r\n[ERROR  ]   S3Error {s3StatusCode = Status {statusCode = 400, statusMessage = \"Bad Request\"}, s3ErrorCode = \"EntityTooLarge\", s3ErrorMessage = \"Your proposed upload exceeds the maximum allowed object size.\", s3ErrorResource = Nothing, s3ErrorHostId = Just \"yGlb-8nYfqfH1xNWwHQkj5W4Og0iw6z-17689Q1KhL0\", s3ErrorAccessKeyId = Nothing, s3ErrorStringToSign = Nothing, s3ErrorBucket = Nothing, s3ErrorEndpointRaw = Nothing, s3ErrorEndpoint = Nothing}\r\n[ERROR  ]   S3Error {s3StatusCode = Status {statusCode = 400, statusMessage = \"Bad Request\"}, s3ErrorCode = \"EntityTooLarge\", s3ErrorMessage = \"Your proposed upload exceeds the maximum allowed object size.\", s3ErrorResource = Nothing, s3ErrorHostId = Just \"OJV0IMmVxyJVm_MwzrmIf2tqa5rPLBD5egD0lvHASME\", s3ErrorAccessKeyId = Nothing, s3ErrorStringToSign = Nothing, s3ErrorBucket = Nothing, s3ErrorEndpointRaw = Nothing, s3ErrorEndpoint = Nothing}\r\n[ERROR  ]   S3Error {s3StatusCode = Status {statusCode = 400, statusMessage = \"Bad Request\"}, s3ErrorCode = \"EntityTooLarge\", s3ErrorMessage = \"Your proposed upload exceeds the maximum allowed object size.\", s3ErrorResource = Nothing, s3ErrorHostId = Just \"OJV0IMmVxyJVm_MwzrmIf2tqa5rPLBD5egD0lvHASME\", s3ErrorAccessKeyId = Nothing, s3ErrorStringToSign = Nothing, s3ErrorBucket = Nothing, s3ErrorEndpointRaw = Nothing, s3ErrorEndpoint = Nothing}\r\n[ERROR  ]   S3Error {s3StatusCode = Status {statusCode = 400, statusMessage = \"Bad Request\"}, s3ErrorCode = \"EntityTooLarge\", s3ErrorMessage = \"Your proposed upload exceeds the maximum allowed object size.\", s3ErrorResource = Nothing, s3ErrorHostId = Just \"5ET_5H2r6x4COTTXPqblraROYCWrQpWrS3IuqZAIvzM\", s3ErrorAccessKeyId = Nothing, s3ErrorStringToSign = Nothing, s3ErrorBucket = Nothing, s3ErrorEndpointRaw = Nothing, s3ErrorEndpoint = Nothing}\r\n[ERROR  ]   S3Error {s3StatusCode = Status {statusCode = 400, statusMessage = \"Bad Request\"}, s3ErrorCode = \"EntityTooLarge\", s3ErrorMessage = \"Your proposed upload exceeds the maximum allowed object size.\", s3ErrorResource = Nothing, s3ErrorHostId = Just \"5m2unNOOp9Ki4rbYA-2rAJEgVi4q8tqJn4fUpJLWwq4\", s3ErrorAccessKeyId = Nothing, s3ErrorStringToSign = Nothing, s3ErrorBucket = Nothing, s3ErrorEndpointRaw = Nothing, s3ErrorEndpoint = Nothing}\r\n[ERROR  ]   S3Error {s3StatusCode = Status {statusCode = 400, statusMessage = \"Bad Request\"}, s3ErrorCode = \"EntityTooLarge\", s3ErrorMessage = \"Your proposed upload exceeds the maximum allowed object size.\", s3ErrorResource = Nothing, s3ErrorHostId = Just \"5m2unNOOp9Ki4rbYA-2rAJEgVi4q8tqJn4fUpJLWwq4\", s3ErrorAccessKeyId = Nothing, s3ErrorStringToSign = Nothing, s3ErrorBucket = Nothing, s3ErrorEndpointRaw = Nothing, s3ErrorEndpoint = Nothing}\r\n[ERROR  ]   S3Error {s3StatusCode = Status {statusCode = 400, statusMessage = \"Bad Request\"}, s3ErrorCode = \"EntityTooLarge\", s3ErrorMessage = \"Your proposed upload exceeds the maximum allowed object size.\", s3ErrorResource = Nothing, s3ErrorHostId = Just \"BQqIg0VfQmpKt1EK4pCjjfP7SwSxcQI3JFBUELbjz-8\", s3ErrorAccessKeyId = Nothing, s3ErrorStringToSign = Nothing, s3ErrorBucket = Nothing, s3ErrorEndpointRaw = Nothing, s3ErrorEndpoint = Nothing}\r\n[ERROR  ]   S3Error {s3StatusCode = Status {statusCode = 400, statusMessage = \"Bad Request\"}, s3ErrorCode = \"EntityTooLarge\", s3ErrorMessage = \"Your proposed upload exceeds the maximum allowed object size.\", s3ErrorResource = Nothing, s3ErrorHostId = Just \"TfULcyinRB-nGBaxZYt72-wkDIkXqsa-NKnNPYV1_k0\", s3ErrorAccessKeyId = Nothing, s3ErrorStringToSign = Nothing, s3ErrorBucket = Nothing, s3ErrorEndpointRaw = Nothing, s3ErrorEndpoint = Nothing}\r\n[ERROR  ]   S3Error {s3StatusCode = Status {statusCode = 400, statusMessage = \"Bad Request\"}, s3ErrorCode = \"EntityTooLarge\", s3ErrorMessage = \"Your proposed upload exceeds the maximum allowed object size.\", s3ErrorResource = Nothing, s3ErrorHostId = Just \"TfULcyinRB-nGBaxZYt72-wkDIkXqsa-NKnNPYV1_k0\", s3ErrorAccessKeyId = Nothing, s3ErrorStringToSign = Nothing, s3ErrorBucket = Nothing, s3ErrorEndpointRaw = Nothing, s3ErrorEndpoint = Nothing}\r\n[ERROR  ]   S3Error {s3StatusCode = Status {statusCode = 400, statusMessage = \"Bad Request\"}, s3ErrorCode = \"EntityTooLarge\", s3ErrorMessage = \"Your proposed upload exceeds the maximum allowed object size.\", s3ErrorResource = Nothing, s3ErrorHostId = Just \"4hZn6VrMX1At4bOE2wcIW-8OOGrb5s5iDNR6cj8TZtA\", s3ErrorAccessKeyId = Nothing, s3ErrorStringToSign = Nothing, s3ErrorBucket = Nothing, s3ErrorEndpointRaw = Nothing, s3ErrorEndpoint = Nothing}\r\n[ERROR  ]   S3Error {s3StatusCode = Status {statusCode = 400, statusMessage = \"Bad Request\"}, s3ErrorCode = \"EntityTooLarge\", s3ErrorMessage = \"Your proposed upload exceeds the maximum allowed object size.\", s3ErrorResource = Nothing, s3ErrorHostId = Just \"VJZqEp3y7vUmnMO7wW80i1sKDzV_CGMATkY2vpuI9wA\", s3ErrorAccessKeyId = Nothing, s3ErrorStringToSign = Nothing, s3ErrorBucket = Nothing, s3ErrorEndpointRaw = Nothing, s3ErrorEndpoint = Nothing}\r\n[ERROR  ]   S3Error {s3StatusCode = Status {statusCode = 400, statusMessage = \"Bad Request\"}, s3ErrorCode = \"EntityTooLarge\", s3ErrorMessage = \"Your proposed upload exceeds the maximum allowed object size.\", s3ErrorResource = Nothing, s3ErrorHostId = Just \"VJZqEp3y7vUmnMO7wW80i1sKDzV_CGMATkY2vpuI9wA\", s3ErrorAccessKeyId = Nothing, s3ErrorStringToSign = Nothing, s3ErrorBucket = Nothing, s3ErrorEndpointRaw = Nothing, s3ErrorEndpoint = Nothing}\r\n[ERROR  ]   S3Error {s3StatusCode = Status {statusCode = 400, statusMessage = \"Bad Request\"}, s3ErrorCode = \"EntityTooLarge\", s3ErrorMessage = \"Your proposed upload exceeds the maximum allowed object size.\", s3ErrorResource = Nothing, s3ErrorHostId = Just \"Gt9zeII8PsH5zMeUMprIrEwS9RVYeHudlBTtT7ji-6o\", s3ErrorAccessKeyId = Nothing, s3ErrorStringToSign = Nothing, s3ErrorBucket = Nothing, s3ErrorEndpointRaw = Nothing, s3ErrorEndpoint = Nothing}\r\n[ERROR  ]   S3Error {s3StatusCode = Status {statusCode = 400, statusMessage = \"Bad Request\"}, s3ErrorCode = \"EntityTooLarge\", s3ErrorMessage = \"Your proposed upload exceeds the maximum allowed object size.\", s3ErrorResource = Nothing, s3ErrorHostId = Just \"VJZqEp3y7vUmnMO7wW80i1sKDzV_CGMATkY2vpuI9wA\", s3ErrorAccessKeyId = Nothing, s3ErrorStringToSign = Nothing, s3ErrorBucket = Nothing, s3ErrorEndpointRaw = Nothing, s3ErrorEndpoint = Nothing}\r\n[ERROR  ]   S3Error {s3StatusCode = Status {statusCode = 400, statusMessage = \"Bad Request\"}, s3ErrorCode = \"EntityTooLarge\", s3ErrorMessage = \"Your proposed upload exceeds the maximum allowed object size.\", s3ErrorResource = Nothing, s3ErrorHostId = Just \"Gt9zeII8PsH5zMeUMprIrEwS9RVYeHudlBTtT7ji-6o\", s3ErrorAccessKeyId = Nothing, s3ErrorStringToSign = Nothing, s3ErrorBucket = Nothing, s3ErrorEndpointRaw = Nothing, s3ErrorEndpoint = Nothing}\r\n[ERROR  ]   S3Error {s3StatusCode = Status {statusCode = 400, statusMessage = \"Bad Request\"}, s3ErrorCode = \"EntityTooLarge\", s3ErrorMessage = \"Your proposed upload exceeds the maximum allowed object size.\", s3ErrorResource = Nothing, s3ErrorHostId = Just \"e2aHald-a5r_AR0DMo-lymSIh--334mbVt3L4EDHTJc\", s3ErrorAccessKeyId = Nothing, s3ErrorStringToSign = Nothing, s3ErrorBucket = Nothing, s3ErrorEndpointRaw = Nothing, s3ErrorEndpoint = Nothing}\r\n[ERROR  ]   S3Error {s3StatusCode = Status {statusCode = 400, statusMessage = \"Bad Request\"}, s3ErrorCode = \"EntityTooLarge\", s3ErrorMessage = \"Your proposed upload exceeds the maximum allowed object size.\", s3ErrorResource = Nothing, s3ErrorHostId = Just \"e2aHald-a5r_AR0DMo-lymSIh--334mbVt3L4EDHTJc\", s3ErrorAccessKeyId = Nothing, s3ErrorStringToSign = Nothing, s3ErrorBucket = Nothing, s3ErrorEndpointRaw = Nothing, s3ErrorEndpoint = Nothing}\r\n[ERROR  ]   S3Error {s3StatusCode = Status {statusCode = 400, statusMessage = \"Bad Request\"}, s3ErrorCode = \"EntityTooLarge\", s3ErrorMessage = \"Your proposed upload exceeds the maximum allowed object size.\", s3ErrorResource = Nothing, s3ErrorHostId = Just \"7aVHXU6xtforOxXfdNBVuuIt4NndfeAhzRuw1R5NLB0\", s3ErrorAccessKeyId = Nothing, s3ErrorStringToSign = Nothing, s3ErrorBucket = Nothing, s3ErrorEndpointRaw = Nothing, s3ErrorEndpoint = Nothing}\r\n[DEBUG  ] Process 29381 exited with return code 1  \r\nPush to 'github':  25%|\u258e| 1.00/4.00 [01:23<04:11, 8CommandError: 'git -c diff.ignoreSubmodules=none annex copy --batch -z --to s3 --fast --json --json-error-messages --json-progress -c annex.dotfiles=true -c annex.retry=3' failed with exitcode 1 under /work/fatx405/projects/envs\r\ngit-annex: copy: 6 failed\r\n```\r\n\r\n</details>\r\n\r\n#### What steps will reproduce the problem?\r\nI initialized the S3 special remote as descriped in a handbook chapter (https://handbook.datalad.org/en/latest/basics/101-139-s3.html) with a script.\r\n\r\n```\r\n    # Initialize S3 special remote in superds and create LZS S3 bucket as sibling\r\n    echo \"Please provide a repository name\"\r\n    read repo\r\n\r\n    echo \"Supply S3 bucket name to create or save to.\"\r\n    echo \"Please adhere to format uke-csi-<individual name>.\"\r\n    echo \"Naming the bucket according to the repository name is recommended. Like uke-csi-<repository name>.\"\r\n    echo \"Add '-test' if you don't want the LZS admins to establish a bucket mirror; e.g. uke-csi-dataset-test.\"\r\n    read bucket\r\n\r\n    echo \"Enter AWS access key\"\r\n    read aws_access\r\n    export AWS_ACCESS_KEY_ID=$aws_access\r\n    echo $AWS_ACCESS_KEY_ID\r\n    \r\n    echo \"Enter AWS secret access key\"\r\n    read aws_secret\r\n    export AWS_SECRET_ACCESS_KEY=$aws_secret\r\n    echo $AWS_SECRET_ACCESS_KEY\r\n\r\n    git annex initremote s3 type=S3 datacenter=s3-uhh encryption=none bucket=$bucket public=no autoenable=true host=s3-uhh.lzs.uni-hamburg.de\r\n\r\n    git annex enableremote s3 publicurl=https://${bucket}.s3-uhh.lzs.uni-hamburg.de\r\n\r\n    datalad create-sibling-github -d . --publish-depends s3 --github-organization csi-hamburg --private -s github $proj\r\n```\r\n\r\n\r\n\r\n#### What version of DataLad are you using (run `datalad --version`)? On what operating system (consider running `datalad wtf`)?\r\n\r\n<details>\r\n<summary>WTF</summary>\r\n\r\n# WTF\r\n## configuration <SENSITIVE, report disabled by configuration>\r\n## credentials \r\n  - keyring: \r\n    - active_backends: \r\n      - PlaintextKeyring with no encyption v.1.0 at /home/fatx405/.local/share/python_keyring/keyring_pass.cfg\r\n    - config_file: /home/fatx405/.config/python_keyring/keyringrc.cfg\r\n    - data_root: /home/fatx405/.local/share/python_keyring\r\n## datalad \r\n  - full_version: 0.14.4\r\n  - version: 0.14.4\r\n## dependencies \r\n  - annexremote: 1.5.0\r\n  - appdirs: 1.4.4\r\n  - boto: 2.49.0\r\n  - cmd:7z: 16.02\r\n  - cmd:annex: 8.20201104-g13bab4f2c\r\n  - cmd:bundled-git: 2.29.2\r\n  - cmd:git: 2.29.2\r\n  - cmd:system-git: 2.29.2\r\n  - cmd:system-ssh: 7.4p1\r\n  - exifread: 2.1.2\r\n  - humanize: 3.2.0\r\n  - iso8601: 0.1.14\r\n  - keyring: 22.0.1\r\n  - keyrings.alt: 4.0.2\r\n  - msgpack: 1.0.2\r\n  - mutagen: 1.41.1\r\n  - requests: 2.25.1\r\n  - wrapt: 1.12.1\r\n## environment \r\n  - LANG: en_US.UTF-8\r\n  - PATH: /work/fatx405/miniconda3/bin:/sw/link/git/2.32.0/bin:/sw/env/system-gcc/singularity/3.5.2-overlayfix/bin:/sw/batch/slurm/19.05.6/bin:/sw/rrz/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin\r\n## extensions \r\n  - container: \r\n    - description: Containerized environments\r\n    - entrypoints: \r\n      - datalad_container.containers_add.ContainersAdd: \r\n        - class: ContainersAdd\r\n        - load_error: None\r\n        - module: datalad_container.containers_add\r\n        - names: \r\n          - containers-add\r\n          - containers_add\r\n      - datalad_container.containers_list.ContainersList: \r\n        - class: ContainersList\r\n        - load_error: None\r\n        - module: datalad_container.containers_list\r\n        - names: \r\n          - containers-list\r\n          - containers_list\r\n      - datalad_container.containers_remove.ContainersRemove: \r\n        - class: ContainersRemove\r\n        - load_error: None\r\n        - module: datalad_container.containers_remove\r\n        - names: \r\n          - containers-remove\r\n          - containers_remove\r\n      - datalad_container.containers_run.ContainersRun: \r\n        - class: ContainersRun\r\n        - load_error: None\r\n        - module: datalad_container.containers_run\r\n        - names: \r\n          - containers-run\r\n          - containers_run\r\n    - load_error: None\r\n    - module: datalad_container\r\n    - version: 1.1.4\r\n  - hirni: \r\n    - description: HIRNI workflows\r\n    - entrypoints: \r\n      - datalad_hirni.commands.dicom2spec.Dicom2Spec: \r\n        - class: Dicom2Spec\r\n        - load_error: None\r\n        - module: datalad_hirni.commands.dicom2spec\r\n        - names: \r\n          - hirni-dicom2spec\r\n          - hirni_dicom2spec\r\n      - datalad_hirni.commands.import_dicoms.ImportDicoms: \r\n        - class: ImportDicoms\r\n        - load_error: None\r\n        - module: datalad_hirni.commands.import_dicoms\r\n        - names: \r\n          - hirni-import-dcm\r\n          - hirni_import_dcm\r\n      - datalad_hirni.commands.spec2bids.Spec2Bids: \r\n        - class: Spec2Bids\r\n        - load_error: None\r\n        - module: datalad_hirni.commands.spec2bids\r\n        - names: \r\n          - hirni-spec2bids\r\n          - hirni_spec2bids\r\n      - datalad_hirni.commands.spec4anything.Spec4Anything: \r\n        - class: Spec4Anything\r\n        - load_error: None\r\n        - module: datalad_hirni.commands.spec4anything\r\n        - names: \r\n          - hirni-spec4anything\r\n          - hirni_spec4anything\r\n    - load_error: None\r\n    - module: datalad_hirni\r\n    - version: 0.0.8\r\n  - metalad: \r\n    - description: DataLad semantic metadata command suite\r\n    - entrypoints: \r\n      - datalad_metalad.aggregate.Aggregate: \r\n        - class: Aggregate\r\n        - load_error: None\r\n        - module: datalad_metalad.aggregate\r\n        - names: \r\n          - meta-aggregate\r\n          - meta_aggregate\r\n      - datalad_metalad.dump.Dump: \r\n        - class: Dump\r\n        - load_error: None\r\n        - module: datalad_metalad.dump\r\n        - names: \r\n          - meta-dump\r\n          - meta_dump\r\n      - datalad_metalad.extract.Extract: \r\n        - class: Extract\r\n        - load_error: None\r\n        - module: datalad_metalad.extract\r\n        - names: \r\n          - meta-extract\r\n          - meta_extract\r\n    - load_error: None\r\n    - module: datalad_metalad\r\n    - version: 0.2.1\r\n  - neuroimaging: \r\n    - description: Neuroimaging tools\r\n    - entrypoints: \r\n      - datalad_neuroimaging.bids2scidata.BIDS2Scidata: \r\n        - class: BIDS2Scidata\r\n        - load_error: None\r\n        - module: datalad_neuroimaging.bids2scidata\r\n        - names: \r\n          - bids2scidata\r\n    - load_error: None\r\n    - module: datalad_neuroimaging\r\n    - version: 0.3.1\r\n  - ukbiobank: \r\n    - description: UKBiobank dataset support\r\n    - entrypoints: \r\n      - datalad_ukbiobank.init.Init: \r\n        - class: Init\r\n        - load_error: None\r\n        - module: datalad_ukbiobank.init\r\n        - names: \r\n          - ukb-init\r\n          - ukb_init\r\n      - datalad_ukbiobank.update.Update: \r\n        - class: Update\r\n        - load_error: None\r\n        - module: datalad_ukbiobank.update\r\n        - names: \r\n          - ukb-update\r\n          - ukb_update\r\n    - load_error: None\r\n    - module: datalad_ukbiobank\r\n    - version: 0.3.2\r\n  - webapp: \r\n    - description: Generic web app support\r\n    - entrypoints: \r\n      - datalad_webapp.WebApp: \r\n        - class: WebApp\r\n        - load_error: None\r\n        - module: datalad_webapp\r\n        - names: \r\n          - webapp\r\n          - webapp\r\n    - load_error: None\r\n    - module: datalad_webapp\r\n    - version: 0.3\r\n## git-annex \r\n  - build flags: \r\n    - Assistant\r\n    - Webapp\r\n    - Pairing\r\n    - Inotify\r\n    - DBus\r\n    - DesktopNotify\r\n    - TorrentParser\r\n    - MagicMime\r\n    - Feeds\r\n    - Testsuite\r\n    - S3\r\n    - WebDAV\r\n  - dependency versions: \r\n    - aws-0.22\r\n    - bloomfilter-2.0.1.0\r\n    - cryptonite-0.26\r\n    - DAV-1.3.4\r\n    - feed-1.3.0.1\r\n    - ghc-8.8.4\r\n    - http-client-0.6.4.1\r\n    - persistent-sqlite-2.10.6.2\r\n    - torrent-10000.1.1\r\n    - uuid-1.3.13\r\n    - yesod-1.6.1.0\r\n  - key/value backends: \r\n    - SHA256E\r\n    - SHA256\r\n    - SHA512E\r\n    - SHA512\r\n    - SHA224E\r\n    - SHA224\r\n    - SHA384E\r\n    - SHA384\r\n    - SHA3_256E\r\n    - SHA3_256\r\n    - SHA3_512E\r\n    - SHA3_512\r\n    - SHA3_224E\r\n    - SHA3_224\r\n    - SHA3_384E\r\n    - SHA3_384\r\n    - SKEIN256E\r\n    - SKEIN256\r\n    - SKEIN512E\r\n    - SKEIN512\r\n    - BLAKE2B256E\r\n    - BLAKE2B256\r\n    - BLAKE2B512E\r\n    - BLAKE2B512\r\n    - BLAKE2B160E\r\n    - BLAKE2B160\r\n    - BLAKE2B224E\r\n    - BLAKE2B224\r\n    - BLAKE2B384E\r\n    - BLAKE2B384\r\n    - BLAKE2BP512E\r\n    - BLAKE2BP512\r\n    - BLAKE2S256E\r\n    - BLAKE2S256\r\n    - BLAKE2S160E\r\n    - BLAKE2S160\r\n    - BLAKE2S224E\r\n    - BLAKE2S224\r\n    - BLAKE2SP256E\r\n    - BLAKE2SP256\r\n    - BLAKE2SP224E\r\n    - BLAKE2SP224\r\n    - SHA1E\r\n    - SHA1\r\n    - MD5E\r\n    - MD5\r\n    - WORM\r\n    - URL\r\n    - X*\r\n  - operating system: linux x86_64\r\n  - remote types: \r\n    - git\r\n    - gcrypt\r\n    - p2p\r\n    - S3\r\n    - bup\r\n    - directory\r\n    - rsync\r\n    - web\r\n    - bittorrent\r\n    - webdav\r\n    - adb\r\n    - tahoe\r\n    - glacier\r\n    - ddar\r\n    - git-lfs\r\n    - httpalso\r\n    - hook\r\n    - external\r\n  - supported repository versions: \r\n    - 8\r\n  - upgrade supported from repository versions: \r\n    - 0\r\n    - 1\r\n    - 2\r\n    - 3\r\n    - 4\r\n    - 5\r\n    - 6\r\n    - 7\r\n  - version: 8.20201104-g13bab4f2c\r\n## location \r\n  - path: /home/fatx405\r\n  - type: directory\r\n## metadata_extractors \r\n  - annex (datalad 0.14.4): \r\n    - distribution: datalad 0.14.4\r\n    - load_error: None\r\n    - module: datalad.metadata.extractors.annex\r\n    - version: None\r\n  - audio (datalad 0.14.4): \r\n    - distribution: datalad 0.14.4\r\n    - load_error: None\r\n    - module: datalad.metadata.extractors.audio\r\n    - version: None\r\n  - bids (datalad-neuroimaging 0.3.1): \r\n    - distribution: datalad-neuroimaging 0.3.1\r\n    - load_error: None\r\n    - module: datalad_neuroimaging.extractors.bids\r\n    - version: None\r\n  - datacite (datalad 0.14.4): \r\n    - distribution: datalad 0.14.4\r\n    - load_error: None\r\n    - module: datalad.metadata.extractors.datacite\r\n    - version: None\r\n  - datalad_core (datalad 0.14.4): \r\n    - distribution: datalad 0.14.4\r\n    - load_error: None\r\n    - module: datalad.metadata.extractors.datalad_core\r\n    - version: None\r\n  - datalad_rfc822 (datalad 0.14.4): \r\n    - distribution: datalad 0.14.4\r\n    - load_error: None\r\n    - module: datalad.metadata.extractors.datalad_rfc822\r\n    - version: None\r\n  - dicom (datalad-neuroimaging 0.3.1): \r\n    - distribution: datalad-neuroimaging 0.3.1\r\n    - load_error: None\r\n    - module: datalad_neuroimaging.extractors.dicom\r\n    - version: None\r\n  - exif (datalad 0.14.4): \r\n    - distribution: datalad 0.14.4\r\n    - load_error: None\r\n    - module: datalad.metadata.extractors.exif\r\n    - version: None\r\n  - frictionless_datapackage (datalad 0.14.4): \r\n    - distribution: datalad 0.14.4\r\n    - load_error: None\r\n    - module: datalad.metadata.extractors.frictionless_datapackage\r\n    - version: None\r\n  - image (datalad 0.14.4): \r\n    - distribution: datalad 0.14.4\r\n    - load_error: None\r\n    - module: datalad.metadata.extractors.image\r\n    - version: None\r\n  - metalad_annex (datalad-metalad 0.2.1): \r\n    - distribution: datalad-metalad 0.2.1\r\n    - load_error: None\r\n    - module: datalad_metalad.extractors.annex\r\n    - version: None\r\n  - metalad_core (datalad-metalad 0.2.1): \r\n    - distribution: datalad-metalad 0.2.1\r\n    - load_error: None\r\n    - module: datalad_metalad.extractors.core\r\n    - version: None\r\n  - metalad_custom (datalad-metalad 0.2.1): \r\n    - distribution: datalad-metalad 0.2.1\r\n    - load_error: None\r\n    - module: datalad_metalad.extractors.custom\r\n    - version: None\r\n  - metalad_runprov (datalad-metalad 0.2.1): \r\n    - distribution: datalad-metalad 0.2.1\r\n    - load_error: None\r\n    - module: datalad_metalad.extractors.runprov\r\n    - version: None\r\n  - nidm (datalad-neuroimaging 0.3.1): \r\n    - distribution: datalad-neuroimaging 0.3.1\r\n    - load_error: None\r\n    - module: datalad_neuroimaging.extractors.nidm\r\n    - version: None\r\n  - nifti1 (datalad-neuroimaging 0.3.1): \r\n    - distribution: datalad-neuroimaging 0.3.1\r\n    - load_error: None\r\n    - module: datalad_neuroimaging.extractors.nifti1\r\n    - version: None\r\n  - xmp (datalad 0.14.4): \r\n    - distribution: datalad 0.14.4\r\n    - load_error: Exempi library not found. [exempi.py:_load_exempi:60]\r\n    - module: datalad.metadata.extractors.xmp\r\n## metadata_indexers \r\n## python \r\n  - implementation: CPython\r\n  - version: 3.7.9\r\n## system \r\n  - distribution: CentOS Linux/7.9.2009/Core\r\n  - encoding: \r\n    - default: utf-8\r\n    - filesystem: utf-8\r\n    - locale.prefered: UTF-8\r\n  - max_path_length: 269\r\n  - name: Linux\r\n  - release: 4.14.240-1.0.33.el7.rrz.x86_64\r\n  - type: posix\r\n  - version: #1 SMP Thu Jul 22 18:29:43 CEST 2021\r\n\r\n</details>\r\n\r\nAs always grateful for any input!\r\n\r\nCheers,\r\nMarvin", "closed_by": {"login": "m-petersen", "id": 39294105, "node_id": "MDQ6VXNlcjM5Mjk0MTA1", "avatar_url": "https://avatars.githubusercontent.com/u/39294105?v=4", "gravatar_id": "", "url": "https://api.github.com/users/m-petersen", "html_url": "https://github.com/m-petersen", "followers_url": "https://api.github.com/users/m-petersen/followers", "following_url": "https://api.github.com/users/m-petersen/following{/other_user}", "gists_url": "https://api.github.com/users/m-petersen/gists{/gist_id}", "starred_url": "https://api.github.com/users/m-petersen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/m-petersen/subscriptions", "organizations_url": "https://api.github.com/users/m-petersen/orgs", "repos_url": "https://api.github.com/users/m-petersen/repos", "events_url": "https://api.github.com/users/m-petersen/events{/privacy}", "received_events_url": "https://api.github.com/users/m-petersen/received_events", "type": "User", "site_admin": false}, "reactions": {"url": "https://api.github.com/repos/datalad/datalad/issues/5890/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/datalad/datalad/issues/5890/timeline", "performed_via_github_app": null}