{"url": "https://api.github.com/repos/datalad/datalad/issues/4003", "repository_url": "https://api.github.com/repos/datalad/datalad", "labels_url": "https://api.github.com/repos/datalad/datalad/issues/4003/labels{/name}", "comments_url": "https://api.github.com/repos/datalad/datalad/issues/4003/comments", "events_url": "https://api.github.com/repos/datalad/datalad/issues/4003/events", "html_url": "https://github.com/datalad/datalad/issues/4003", "id": 547776420, "node_id": "MDU6SXNzdWU1NDc3NzY0MjA=", "number": 4003, "title": "idea: datalad/git-annex aware streaming/random access access to files content", "user": {"login": "yarikoptic", "id": 39889, "node_id": "MDQ6VXNlcjM5ODg5", "avatar_url": "https://avatars.githubusercontent.com/u/39889?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yarikoptic", "html_url": "https://github.com/yarikoptic", "followers_url": "https://api.github.com/users/yarikoptic/followers", "following_url": "https://api.github.com/users/yarikoptic/following{/other_user}", "gists_url": "https://api.github.com/users/yarikoptic/gists{/gist_id}", "starred_url": "https://api.github.com/users/yarikoptic/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yarikoptic/subscriptions", "organizations_url": "https://api.github.com/users/yarikoptic/orgs", "repos_url": "https://api.github.com/users/yarikoptic/repos", "events_url": "https://api.github.com/users/yarikoptic/events{/privacy}", "received_events_url": "https://api.github.com/users/yarikoptic/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 63778592, "node_id": "MDU6TGFiZWw2Mzc3ODU5Mg==", "url": "https://api.github.com/repos/datalad/datalad/labels/enhancement", "name": "enhancement", "color": "0052cc", "default": true, "description": ""}, {"id": 655634847, "node_id": "MDU6TGFiZWw2NTU2MzQ4NDc=", "url": "https://api.github.com/repos/datalad/datalad/labels/good-for-hackathon", "name": "good-for-hackathon", "color": "fbca04", "default": false, "description": ""}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 13, "created_at": "2020-01-09T23:14:51Z", "updated_at": "2021-09-16T19:22:58Z", "closed_at": null, "author_association": "MEMBER", "active_lock_reason": null, "body": "- ATM we have `AutomagicIO` which provide somewhat ad-hoc/ugly (relies on monkey patching \"known\" open calls) hack to automagically `datalad get` files being accessed\r\n- there was another \"idea\" (desire) expressed to build atop some batched access to datasets (or files) content: https://github.com/datalad/datalad/issues/3121\r\n- `git annex` contains information about where each file count be accessed from, often including URLs via standard protocols (http{,s}, ftp, etc)\r\n- for many \"remote\" access mechanisms (http{,s} urls) 3rd party existing tools could be utilized to provide streaming or even random access using requests for specific bytes ranges (see https://github.com/dandi/infrastructure/issues/12 for pilot attempt using httpfs2 and a pointer to [astreamfs](https://gitlab.com/BylonAkila/astreamfs) tuned for streaming access).\r\n\r\nTwo modes could be envisioned\r\n\r\n- \"fetch and forget\", with some optional cache for most recent/frequent requests -- would just fetch the content without anyhow affecting local `git-annex` store of the dataset\r\n- \"fetch and get\" -- if user requests enough (e.g. `> 90%`) of the file content, we could as well fetch the rest, and make that annex key actually available in the local git-annex repository. Could be implemented by fetching initiating a sparse file, filling out its content as requests are coming in, and finalizing (fetching the rest and moving it under git-annex key) whenever file is closed.\r\n- \"sparse cache\" -- cache requested/downloaded data (as [sparse files](https://en.wikipedia.org/wiki/Sparse_file) for original keys) in a local cache (with mtime/atime adjusted accordingly, so cache could be efficiently trimmed whenever needed)\r\n\r\nI think it should be quite easy to achieve basic pilot (for \"fetch and forget\") implementation for Linux, by creating a datalad FUSE filesystem, which would \r\n\r\n- talk to git-annex to figure out where a key is available from\r\n- initiating 3rd party tools (configurable -- httpfs2 or astreamfs for http, sshfs for ssh) to provide access to the content behind url\r\n- functionality relying on random access of compressed files with indexes, such as .xz, could then seamlessly access specific files within those archives without requiring to download the entire archive  (see https://github.com/datalad/datalad/issues/373)\r\n\r\n\r\n\"user interface\" to invoke it could be the same \"datalad shell\" we briefly discussed in the past (https://github.com/datalad/datalad/issues/2063), e.g. `datalad shell --mode=fs [COMMAND ...]` so that if there is no `[COMMAND ...]` - opens a shell, otherwise just executes the `COMMAND ...` under such \"FUSE\" control.  In Python could be a context manager, so ```with dl.shell(mode='fs'):``` (if possible to achieve in the same process ;) ).  Then, any (possibly domain specific) batching approaches desired in https://github.com/datalad/datalad/issues/3121 could be implemented on top.\r\n\r\n", "closed_by": null, "reactions": {"url": "https://api.github.com/repos/datalad/datalad/issues/4003/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/datalad/datalad/issues/4003/timeline", "performed_via_github_app": null}