{"url": "https://api.github.com/repos/datalad/datalad/issues/5053", "repository_url": "https://api.github.com/repos/datalad/datalad", "labels_url": "https://api.github.com/repos/datalad/datalad/issues/5053/labels{/name}", "comments_url": "https://api.github.com/repos/datalad/datalad/issues/5053/comments", "events_url": "https://api.github.com/repos/datalad/datalad/issues/5053/events", "html_url": "https://github.com/datalad/datalad/issues/5053", "id": 724687593, "node_id": "MDU6SXNzdWU3MjQ2ODc1OTM=", "number": 5053, "title": "Progress reporting for git-annex checksuming", "user": {"login": "mih", "id": 136479, "node_id": "MDQ6VXNlcjEzNjQ3OQ==", "avatar_url": "https://avatars.githubusercontent.com/u/136479?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mih", "html_url": "https://github.com/mih", "followers_url": "https://api.github.com/users/mih/followers", "following_url": "https://api.github.com/users/mih/following{/other_user}", "gists_url": "https://api.github.com/users/mih/gists{/gist_id}", "starred_url": "https://api.github.com/users/mih/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mih/subscriptions", "organizations_url": "https://api.github.com/users/mih/orgs", "repos_url": "https://api.github.com/users/mih/repos", "events_url": "https://api.github.com/users/mih/events{/privacy}", "received_events_url": "https://api.github.com/users/mih/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 390153891, "node_id": "MDU6TGFiZWwzOTAxNTM4OTE=", "url": "https://api.github.com/repos/datalad/datalad/labels/UX", "name": "UX", "color": "0052cc", "default": false, "description": "user experience"}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-10-19T14:34:35Z", "updated_at": "2020-10-19T14:34:35Z", "closed_at": null, "author_association": "MEMBER", "active_lock_reason": null, "body": "I am working with datasets that have file sizes beyond the 1TB mark. While there is nice progress reporting in the download phase there is none for the checksum computation that takes place after a successful download. In the case that motivates me to write this issue, the hashing is CPU-bound at ~500M/s and takes roughly 40min. The whole operation to obtain a single file took ~90min, with download also taking place at about 500M/s.", "closed_by": null, "reactions": {"url": "https://api.github.com/repos/datalad/datalad/issues/5053/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/datalad/datalad/issues/5053/timeline", "performed_via_github_app": null}