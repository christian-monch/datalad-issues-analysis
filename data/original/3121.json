{"url": "https://api.github.com/repos/datalad/datalad/issues/3121", "repository_url": "https://api.github.com/repos/datalad/datalad", "labels_url": "https://api.github.com/repos/datalad/datalad/issues/3121/labels{/name}", "comments_url": "https://api.github.com/repos/datalad/datalad/issues/3121/comments", "events_url": "https://api.github.com/repos/datalad/datalad/issues/3121/events", "html_url": "https://github.com/datalad/datalad/issues/3121", "id": 399102765, "node_id": "MDU6SXNzdWUzOTkxMDI3NjU=", "number": 3121, "title": "idea: provide a plugin interface to allow implementation of data loading and \"streaming\"", "user": {"login": "satyaog", "id": 6757005, "node_id": "MDQ6VXNlcjY3NTcwMDU=", "avatar_url": "https://avatars.githubusercontent.com/u/6757005?v=4", "gravatar_id": "", "url": "https://api.github.com/users/satyaog", "html_url": "https://github.com/satyaog", "followers_url": "https://api.github.com/users/satyaog/followers", "following_url": "https://api.github.com/users/satyaog/following{/other_user}", "gists_url": "https://api.github.com/users/satyaog/gists{/gist_id}", "starred_url": "https://api.github.com/users/satyaog/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/satyaog/subscriptions", "organizations_url": "https://api.github.com/users/satyaog/orgs", "repos_url": "https://api.github.com/users/satyaog/repos", "events_url": "https://api.github.com/users/satyaog/events{/privacy}", "received_events_url": "https://api.github.com/users/satyaog/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 736433505, "node_id": "MDU6TGFiZWw3MzY0MzM1MDU=", "url": "https://api.github.com/repos/datalad/datalad/labels/severity-wishlist", "name": "severity-wishlist", "color": "ffeeee", "default": false, "description": "feature request, or bugs that are very difficult to fix due to design issues"}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2019-01-14T22:32:09Z", "updated_at": "2021-09-21T18:44:39Z", "closed_at": null, "author_association": "NONE", "active_lock_reason": null, "body": "#### What is the problem?\r\nGeneral information\r\n#### What steps will reproduce the problem?\r\nN/A\r\n#### What version of DataLad are you using (run `datalad --version`)? On what operating system (consider running `datalad wtf`)?\r\nN/A\r\n#### Is there anything else that would be useful to know in this context?\r\nNo\r\n#### Have you had any success using DataLad before? (to assess your expertise/prior luck.  We would welcome your testimonial additions to https://github.com/datalad/datalad/wiki/Testimonials as well)\r\nN/A\r\n\r\nHi,\r\n\r\nI'm working at Mila, an AI research institute, and we are looking for solutions to distribute and load the datasets we have in our cluster mostly, for our researchers at first. The distribution would be internally in the cluster and externally to other compute graphs depending on the workflow of each researcher.\r\n\r\nWe looked into DataLad and we really liked that it's harnessing the flexibility and power of git/git-annex, the ability easily track the updates on a dataset and the organization of datasets and super-datasets.\r\n\r\nFeatures that for us would be useful and in which we don't seam to find a fit in DataLad in it's current state would be regarding data loading and \"streaming\". Data loading would be in the form of a simple plugin interface that abstracts the load of an entire dataset, the load by batch by returning an iterator of batches of random entries from the dataset or the load of a specific partition of the dataset, if the dataset can be separated in partitions. The idea behind this would be to provide a simple way for our users to implement the actual loading of data using the library or method that they prefer without having to worry too much about the plumbing.\r\n\r\nA \"streaming\" feature could be very useful in the case the dataset is very big and the user doesn't want to wait until the complete download of the dataset before starting an experiment. This would be very dependent on the data format but if it allows random access, the user could download a batch of random entries and pre-download the next batch while the processing of the previous is happening. I imagine however that this would be highly dependant on the features offered by git-annex as it would need to have an interface allowing to specify which bytes to download.\r\n\r\nPlease let me know what you think of that and to which extend it could fit in the vision of DataLad?\r\n\r\nThank you", "closed_by": null, "reactions": {"url": "https://api.github.com/repos/datalad/datalad/issues/3121/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/datalad/datalad/issues/3121/timeline", "performed_via_github_app": null}