{"url": "https://api.github.com/repos/datalad/datalad/issues/4009", "repository_url": "https://api.github.com/repos/datalad/datalad", "labels_url": "https://api.github.com/repos/datalad/datalad/issues/4009/labels{/name}", "comments_url": "https://api.github.com/repos/datalad/datalad/issues/4009/comments", "events_url": "https://api.github.com/repos/datalad/datalad/issues/4009/events", "html_url": "https://github.com/datalad/datalad/issues/4009", "id": 548267506, "node_id": "MDU6SXNzdWU1NDgyNjc1MDY=", "number": 4009, "title": "Suggestions on how to use DataLad with very large datasets", "user": {"login": "satyaog", "id": 6757005, "node_id": "MDQ6VXNlcjY3NTcwMDU=", "avatar_url": "https://avatars.githubusercontent.com/u/6757005?v=4", "gravatar_id": "", "url": "https://api.github.com/users/satyaog", "html_url": "https://github.com/satyaog", "followers_url": "https://api.github.com/users/satyaog/followers", "following_url": "https://api.github.com/users/satyaog/following{/other_user}", "gists_url": "https://api.github.com/users/satyaog/gists{/gist_id}", "starred_url": "https://api.github.com/users/satyaog/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/satyaog/subscriptions", "organizations_url": "https://api.github.com/users/satyaog/orgs", "repos_url": "https://api.github.com/users/satyaog/repos", "events_url": "https://api.github.com/users/satyaog/events{/privacy}", "received_events_url": "https://api.github.com/users/satyaog/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 63778594, "node_id": "MDU6TGFiZWw2Mzc3ODU5NA==", "url": "https://api.github.com/repos/datalad/datalad/labels/question", "name": "question", "color": "fbca04", "default": true, "description": "Issue asks a question rather than reporting a problem"}, {"id": 1810420265, "node_id": "MDU6TGFiZWwxODEwNDIwMjY1", "url": "https://api.github.com/repos/datalad/datalad/labels/answered", "name": "answered", "color": "fbca04", "default": false, "description": "An answer to a question-type issue was provided"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-01-10T20:09:10Z", "updated_at": "2020-03-04T18:01:54Z", "closed_at": "2020-03-04T18:01:44Z", "author_association": "NONE", "active_lock_reason": null, "body": "#### What is the problem?\r\nDataLad (git) with a large number of files (1.5M or 10-20M) is slow. I would like to have recommendations how to efficiently use DataLad with a large number of files or what should be my expectation in that context.\r\n#### What steps will reproduce the problem?\r\n1.5M files for 150GB takes more than 48h to index and almost the same time to move from one annex remote to another with annex.thin = true and annex.hardlink = true.\r\n\r\n8M files for 1.3TB took more than 4 weeks to complete indexing and it unfortunately failed at the end (I actually have to check why). I have to say that there was a nice and ionice applied to the process but the machine was not under other heavy use for a long period of time.\r\n#### What version of DataLad are you using (run `datalad --version`)? On what operating system (consider running `datalad wtf`)?\r\nDataLad 0.11.8, git-annex version: 7.20190819-ge4cecf8, git-annex local version 5\r\n#### Is there anything else that would be useful to know in this context?\r\nFor the 1.5M files, I've tried to git-annex files per directories (1000 directories each containing 1500 files) without better results.\r\n#### Have you had any success using DataLad before? (to assess your expertise/prior luck.  We would welcome your testimonial additions to https://github.com/datalad/datalad/wiki/Testimonials as well)\r\nI've successfully used DataLad to annex and track preprocessing steps on smaller datasets (10000s of files) using datalad run or annexed few big files (3 of 100GB).\r\n\r\nBased on git limitation explained in #17, I'm tempted to use DataLad with archives of files instead of individual files and use datalad rerun to store the extraction commands. However, my current understanding is that I would have to write an empty commit myself (since datalad doesn't store empty commits) with the json needed for datalad rerun or maybe change the content of single file.\r\n\r\nI haven't tested using annex.tune as mentioned in #32 but I'm not sure yet how to tune an existing repository (need to look deeper) or what should be my expectations in terms of performance gain.", "closed_by": {"login": "yarikoptic", "id": 39889, "node_id": "MDQ6VXNlcjM5ODg5", "avatar_url": "https://avatars.githubusercontent.com/u/39889?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yarikoptic", "html_url": "https://github.com/yarikoptic", "followers_url": "https://api.github.com/users/yarikoptic/followers", "following_url": "https://api.github.com/users/yarikoptic/following{/other_user}", "gists_url": "https://api.github.com/users/yarikoptic/gists{/gist_id}", "starred_url": "https://api.github.com/users/yarikoptic/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yarikoptic/subscriptions", "organizations_url": "https://api.github.com/users/yarikoptic/orgs", "repos_url": "https://api.github.com/users/yarikoptic/repos", "events_url": "https://api.github.com/users/yarikoptic/events{/privacy}", "received_events_url": "https://api.github.com/users/yarikoptic/received_events", "type": "User", "site_admin": false}, "reactions": {"url": "https://api.github.com/repos/datalad/datalad/issues/4009/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/datalad/datalad/issues/4009/timeline", "performed_via_github_app": null}