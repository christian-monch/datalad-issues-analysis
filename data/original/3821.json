{"url": "https://api.github.com/repos/datalad/datalad/issues/3821", "repository_url": "https://api.github.com/repos/datalad/datalad", "labels_url": "https://api.github.com/repos/datalad/datalad/issues/3821/labels{/name}", "comments_url": "https://api.github.com/repos/datalad/datalad/issues/3821/comments", "events_url": "https://api.github.com/repos/datalad/datalad/issues/3821/events", "html_url": "https://github.com/datalad/datalad/pull/3821", "id": 509968106, "node_id": "MDExOlB1bGxSZXF1ZXN0MzMwNDEwMTI1", "number": 3821, "title": "ENH: Standardized way to add examples to commands", "user": {"login": "adswa", "id": 29738718, "node_id": "MDQ6VXNlcjI5NzM4NzE4", "avatar_url": "https://avatars.githubusercontent.com/u/29738718?v=4", "gravatar_id": "", "url": "https://api.github.com/users/adswa", "html_url": "https://github.com/adswa", "followers_url": "https://api.github.com/users/adswa/followers", "following_url": "https://api.github.com/users/adswa/following{/other_user}", "gists_url": "https://api.github.com/users/adswa/gists{/gist_id}", "starred_url": "https://api.github.com/users/adswa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/adswa/subscriptions", "organizations_url": "https://api.github.com/users/adswa/orgs", "repos_url": "https://api.github.com/users/adswa/repos", "events_url": "https://api.github.com/users/adswa/events{/privacy}", "received_events_url": "https://api.github.com/users/adswa/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-10-21T13:36:23Z", "updated_at": "2020-12-18T07:32:15Z", "closed_at": "2019-10-29T11:36:41Z", "author_association": "MEMBER", "active_lock_reason": null, "pull_request": {"url": "https://api.github.com/repos/datalad/datalad/pulls/3821", "html_url": "https://github.com/datalad/datalad/pull/3821", "diff_url": "https://github.com/datalad/datalad/pull/3821.diff", "patch_url": "https://github.com/datalad/datalad/pull/3821.patch"}, "body": "Following up on #3757, I'm drafting a standardized way to include examples into the docstrings or help messages of commands, and I would appreciate critical feedback.\r\n\r\nMy idea was to mimick the current way in which `_params_` within each command class holds the commands parameters and has custom functions to add them to the parser or the docstring.\r\n\r\nI propose to have `_examples_` as a list of dictionaries, with each dictionary being one command example. The key `text` holds a description of the example, and `code_py` or `code_cmd` keys holds code snippets.\r\n```python\r\n    _examples_ = [\r\n        dict(text=\"\"\"Apply the text2git procedure upon creation of a dataset\"\"\",\r\n             code_py=\"create(path='mydataset', cfg_proc='text2git')\",\r\n             code_cmd=\"datalad create -c text2git mydataset\"),\r\n```\r\n\r\nBased on `_examples_`, additional functions in [`build_doc`](https://github.com/datalad/datalad/blob/3f1652d2da63d3e30f9c67db7a98528c1cf757a0/datalad/interface/base.py#L363) and [`setup_parser`](https://github.com/datalad/datalad/blob/3f1652d2da63d3e30f9c67db7a98528c1cf757a0/datalad/cmdline/main.py#L87) build the example and append it to the docstring or the help message. This is how it currently looks for me for \r\n\r\n<details><summary>cmd line help </summary>\r\n\r\n```sh\r\n\r\ndatalad install --help\r\nUsage: datalad install [-h] [-s SOURCE] [-d DATASET] [-g] [-D DESCRIPTION]\r\n                       [-r] [-R LEVELS] [--nosave] [--reckless] [-J NJOBS]\r\n                       [PATH [PATH ...]]\r\n\r\nInstall a dataset from a (remote) source.\r\n\r\nThis command creates a local sibling of an existing dataset from a\r\n(remote) location identified via a URL or path. Optional recursion into\r\npotential subdatasets, and download of all referenced data is supported.\r\nThe new dataset can be optionally registered in an existing\r\nsuperdataset by identifying it via the DATASET argument (the new\r\ndataset's path needs to be located within the superdataset for that).\r\n\r\nIt is recommended to provide a brief description to label the dataset's\r\nnature *and* location, e.g. \"Michael's music on black laptop\". This helps\r\nhumans to identify data locations in distributed scenarios.  By default an\r\nidentifier comprised of user and machine name, plus path will be generated.\r\n\r\nWhen only partial dataset content shall be obtained, it is recommended to\r\nuse this command without the GET-DATA flag, followed by a\r\n`get` operation to obtain the desired data.\r\n\r\nNOTE\r\n  Power-user info: This command uses git clone, and\r\n  git annex init to prepare the dataset. Registering to a\r\n  superdataset is performed via a git submodule add operation\r\n  in the discovered superdataset.\r\n\r\n*Examples*\r\nInstall a dataset from Github into the current directory:\r\n\r\n   % datalad install https://github.com/datalad-datasets/longnow-podcasts.git\r\n\r\nInstall a dataset as a subdataset into the current dataset:\r\n\r\n   % datalad install -d . --source='https://github.com/datalad-datasets/longnow-podcasts.git'\r\n\r\nInstall a dataset, and get all contents right away:\r\n\r\n   % datalad install --get-data --source https://github.com/datalad-datasets/longnow-podcasts.git\r\n\r\nInstall a dataset with all its subdatasets:\r\n\r\n   % datalad install https://github.com/datalad-datasets/longnow-podcasts.git --recursive\r\n\r\n*Arguments*\r\n  PATH                  path/name of the installation target. If no PATH is\r\n                        provided a destination path will be derived from a\r\n                        source URL similar to git clone. [Default: None]\r\n\r\n*Options*\r\n  -h, --help, --help-np\r\n                        show this help message. --help-np forcefully disables\r\n                        the use of a pager for displaying the help message\r\n  -s SOURCE, --source SOURCE\r\n                        URL or local path of the installation source.\r\n                        Constraints: value must be a string [Default: None]\r\n  -d DATASET, --dataset DATASET\r\n                        specify the dataset to perform the install operation\r\n                        on. If no dataset is given, an attempt is made to\r\n                        identify the dataset in a parent directory of the\r\n                        current working directory and/or the PATH given.\r\n                        Constraints: Value must be a Dataset or a valid\r\n                        identifier of a Dataset (e.g. a path) [Default: None]\r\n  -g, --get-data        if given, obtain all data content too. [Default:\r\n                        False]\r\n  -D DESCRIPTION, --description DESCRIPTION\r\n                        short description to use for a dataset location. Its\r\n                        primary purpose is to help humans to identify a\r\n                        dataset copy (e.g., \"mike's dataset on lab server\").\r\n                        Note that when a dataset is published, this\r\n                        information becomes available on the remote side.\r\n                        Constraints: value must be a string [Default: None]\r\n  -r, --recursive       if set, recurse into potential subdataset. [Default:\r\n                        False]\r\n  -R LEVELS, --recursion-limit LEVELS\r\n                        limit recursion into subdataset to the given number of\r\n                        levels. Constraints: value must be convertible to type\r\n                        'int' [Default: None]\r\n  --nosave              by default all modifications to a dataset are\r\n                        immediately saved. Giving this option will disable\r\n                        this behavior. [Default: True]\r\n  --reckless            Set up the dataset to be able to obtain content in the\r\n                        cheapest/fastest possible way, even if this poses a\r\n                        potential risk the data integrity (e.g. hardlink files\r\n                        from a local clone of the dataset). Use with care, and\r\n                        limit to \"read-only\" use cases. With this flag the\r\n                        installed dataset will be marked as untrusted.\r\n                        [Default: False]\r\n  -J NJOBS, --jobs NJOBS\r\n                        how many parallel jobs (where possible) to use.\r\n                        Constraints: value must be convertible to type 'int',\r\n                        or value must be one of ('auto',) [Default: 'auto']\r\n```\r\n</details>\r\n\r\nand for a \r\n<details><summary>Python help</summary>\r\n\r\n```python\r\n\r\nHelp on function __call__ in module datalad.core.local.create:\r\n\r\nSignature:\r\ncreate(\r\n    path=None,\r\n    initopts=None,\r\n    force=False,\r\n    description=None,\r\n    dataset=None,\r\n    no_annex=False,\r\n    fake_dates=False,\r\n    cfg_proc=None,\r\n)\r\nDocstring:\r\nCreate a new dataset from scratch.\r\n\r\nThis command initializes a new dataset at a given location, or the\r\ncurrent directory. The new dataset can optionally be registered in an\r\nexisting superdataset (the new dataset's path needs to be located\r\nwithin the superdataset for that, and the superdataset needs to be given\r\nexplicitly via `dataset`). It is recommended\r\nto provide a brief description to label the dataset's nature *and*\r\nlocation, e.g. \"Michael's music on black laptop\". This helps humans to\r\nidentify data locations in distributed scenarios.  By default an identifier\r\ncomprised of user and machine name, plus path will be generated.\r\n\r\nThis command only creates a new dataset, it does not add existing content\r\nto it, even if the target directory already contains additional files or\r\ndirectories.\r\n\r\nPlain Git repositories can be created via the `no_annex` flag.\r\nHowever, the result will not be a full dataset, and, consequently,\r\nnot all features are supported (e.g. a description).\r\n\r\nTo create a local version of a remote dataset use the\r\n:func:`~datalad.api.install` command instead.\r\n\r\n.. note::\r\n  Power-user info: This command uses :command:`git init` and\r\n  :command:`git annex init` to prepare the new dataset. Registering to a\r\n  superdataset is performed via a :command:`git submodule add` operation\r\n  in the discovered superdataset.\r\n\r\nExamples\r\n--------\r\nCreate a dataset 'mydataset' in the current directory::\r\n\r\n   create(path='mydataset')\r\n\r\nApply the text2git procedure upon creation of a dataset::\r\n\r\n   create(path='mydataset', cfg_proc='text2git')\r\n\r\nCreate a subdataset in the root of an existing dataset::\r\n\r\n   create(dataset='.', path='mysubdataset')\r\n\r\nCreate a dataset in an existing, non-empty directory::\r\n\r\n   create(force=True, path='.')\r\n\r\nCreate a plain Git repository::\r\n\r\n   create(path='mydataset', no_annex=True)\r\n\r\n\r\nParameters\r\n----------\r\npath : str or Dataset or None, optional\r\n  path where the dataset shall be created, directories will be created\r\n  as necessary. If no location is provided, a dataset will be created\r\n  in the current working directory. Either way the command will error\r\n  if the target directory is not empty. Use `force` to create a\r\n  dataset in a non-empty directory. [Default: None]\r\ninitopts\r\n  options to pass to :command:`git init`. Options can be given as a\r\n  list of command line arguments or as a GitPython-style option\r\n  dictionary. Note that not all options will lead to viable results.\r\n  For example '--bare' will not yield a repository where DataLad can\r\n  adjust files in its worktree. [Default: None]\r\nforce : bool, optional\r\n  enforce creation of a dataset in a non-empty directory. [Default:\r\n  False]\r\ndescription : str or None, optional\r\n  short description to use for a dataset location. Its primary purpose\r\n  is to help humans to identify a dataset copy (e.g., \"mike's dataset\r\n  on lab server\"). Note that when a dataset is published, this\r\n  information becomes available on the remote side. [Default: None]\r\ndataset : Dataset or None, optional\r\n  specify the dataset to perform the create operation on. If a dataset\r\n  is given, a new subdataset will be created in it. [Default: None]\r\nno_annex : bool, optional\r\n  if set, a plain Git repository will be created without any annex.\r\n  [Default: False]\r\nfake_dates : bool, optional\r\n  Configure the repository to use fake dates. The date for a new\r\n  commit will be set to one second later than the latest commit in the\r\n  repository. This can be used to anonymize dates. [Default: False]\r\ncfg_proc\r\n  Run cfg_PROC procedure(s) (can be specified multiple times) on the\r\n  created dataset. Use `run_procedure(discover=True)` to get a list of\r\n  available procedures, such as cfg_text2git. [Default: None]\r\non_failure : {'ignore', 'continue', 'stop'}, optional\r\n  behavior to perform on failure: 'ignore' any failure is reported,\r\n  but does not cause an exception; 'continue' if any failure occurs an\r\n  exception will be raised at the end, but processing other actions\r\n  will continue for as long as possible; 'stop': processing will stop\r\n  on first failure and an exception is raised. A failure is any result\r\n  with status 'impossible' or 'error'. Raised exception is an\r\n  IncompleteResultsError that carries the result dictionaries of the\r\n  failures in its `failed` attribute. [Default: 'continue']\r\nproc_post\r\n  Like `proc_pre`, but procedures are executed after the main command\r\n  has finished. [Default: None]\r\nproc_pre\r\n  DataLad procedure to run prior to the main command. The argument a\r\n  list of lists with procedure names and optional arguments.\r\n  Procedures are called in the order their are given in this list. It\r\n  is important to provide the respective target dataset to run a\r\n  procedure on as the `dataset` argument of the main command.\r\n  [Default: None]\r\nresult_filter : callable or None, optional\r\n  if given, each to-be-returned status dictionary is passed to this\r\n  callable, and is only returned if the callable's return value does\r\n  not evaluate to False or a ValueError exception is raised. If the\r\n  given callable supports `**kwargs` it will additionally be passed\r\n  the keyword arguments of the original API call. [Default: None]\r\nresult_renderer : {'default', 'json', 'json_pp', 'tailored'} or None, optional\r\n  format of return value rendering on stdout. [Default: None]\r\nresult_xfm : {'datasets', 'successdatasets-or-none', 'paths', 'relpaths', 'metadata'} or callable or None, optional\r\n  if given, each to-be-returned result status dictionary is passed to\r\n  this callable, and its return value becomes the result instead. This\r\n  is different from `result_filter`, as it can perform arbitrary\r\n  transformation of the result value. This is mostly useful for top-\r\n  level command invocations that need to provide the results in a\r\n  particular format. Instead of a callable, a label for a pre-crafted\r\n  result transformation can be given. [Default: None]\r\nreturn_type : {'generator', 'list', 'item-or-list'}, optional\r\n  return value behavior switch. If 'item-or-list' a single value is\r\n  returned instead of a one-item return value list, or a list in case\r\n  of multiple return values. `None` is return in case of an empty\r\n  list. [Default: 'list']\r\nFile:      ~/repos/datalad/datalad/core/local/create.py\r\nType:      FunctionWrapper\r\n\r\n```\r\n</details>\r\n\r\nNote that in the docstrings, examples use the simple ``::`` rst markup which I believe gets rendered by Sphinx into code markup\r\n\r\n```\r\nCreate a subdataset in the root of an existing dataset::\r\n    \r\n   create(dataset='.', path='mysubdataset')\r\n```\r\n\r\nIf you have any thoughts on this, please let me know. I will in any case draft examples for hopefully all commands.\r\n\r\n### TODO\r\n\r\nThe dictionary data structure can be expanded with other keys, such as ``setup_py``/``setup_cmd`` and ``teardown_py``/``teardown_cmd``. These keys could hold the relevant code to run the command example and clean up afterwards, e.g.\r\n\r\n```python\r\n        dict(text=\"\"\"Create a dataset 'mydataset' in the current directory\"\"\",\r\n             code_py=\"create(path='mydataset')\",\r\n             code_cmd=\"datalad create mydataset\",\r\n             setup_py=\"python -c 'from datalad.api import create, remove'\",\r\n             setup_cmd=\"datalad create mydataset\",\r\n             teardown_py=\"remove(path='mydataset')\",\r\n             teardown_cmd=\"datalad remove mydataset\"),\r\n```\r\n\r\nThis would make it possible to write functions that build the examples, test whether they run, and tear down what an example executed.\r\n", "closed_by": {"login": "mih", "id": 136479, "node_id": "MDQ6VXNlcjEzNjQ3OQ==", "avatar_url": "https://avatars.githubusercontent.com/u/136479?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mih", "html_url": "https://github.com/mih", "followers_url": "https://api.github.com/users/mih/followers", "following_url": "https://api.github.com/users/mih/following{/other_user}", "gists_url": "https://api.github.com/users/mih/gists{/gist_id}", "starred_url": "https://api.github.com/users/mih/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mih/subscriptions", "organizations_url": "https://api.github.com/users/mih/orgs", "repos_url": "https://api.github.com/users/mih/repos", "events_url": "https://api.github.com/users/mih/events{/privacy}", "received_events_url": "https://api.github.com/users/mih/received_events", "type": "User", "site_admin": false}, "reactions": {"url": "https://api.github.com/repos/datalad/datalad/issues/3821/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/datalad/datalad/issues/3821/timeline", "performed_via_github_app": null}