{"url": "https://api.github.com/repos/datalad/datalad/issues/1671", "repository_url": "https://api.github.com/repos/datalad/datalad", "labels_url": "https://api.github.com/repos/datalad/datalad/issues/1671/labels{/name}", "comments_url": "https://api.github.com/repos/datalad/datalad/issues/1671/comments", "events_url": "https://api.github.com/repos/datalad/datalad/issues/1671/events", "html_url": "https://github.com/datalad/datalad/issues/1671", "id": 246384380, "node_id": "MDU6SXNzdWUyNDYzODQzODA=", "number": 1671, "title": "New commands to provide recursive checkout and merge", "user": {"login": "yarikoptic", "id": 39889, "node_id": "MDQ6VXNlcjM5ODg5", "avatar_url": "https://avatars.githubusercontent.com/u/39889?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yarikoptic", "html_url": "https://github.com/yarikoptic", "followers_url": "https://api.github.com/users/yarikoptic/followers", "following_url": "https://api.github.com/users/yarikoptic/following{/other_user}", "gists_url": "https://api.github.com/users/yarikoptic/gists{/gist_id}", "starred_url": "https://api.github.com/users/yarikoptic/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yarikoptic/subscriptions", "organizations_url": "https://api.github.com/users/yarikoptic/orgs", "repos_url": "https://api.github.com/users/yarikoptic/repos", "events_url": "https://api.github.com/users/yarikoptic/events{/privacy}", "received_events_url": "https://api.github.com/users/yarikoptic/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 63778592, "node_id": "MDU6TGFiZWw2Mzc3ODU5Mg==", "url": "https://api.github.com/repos/datalad/datalad/labels/enhancement", "name": "enhancement", "color": "0052cc", "default": true, "description": ""}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-07-28T15:38:36Z", "updated_at": "2017-10-08T07:16:53Z", "closed_at": null, "author_association": "MEMBER", "active_lock_reason": null, "body": "Openfmri datasets grow with a number of derivatives added to them.  Some of the derivatives are large in # of files.  Example is https://openfmri.org/dataset/ds000030/ .  That is why it would make more sense if such derivatives as freesurfer (or may be any derivative) were placed into a dedicated subdataset by the crawler.\r\n\r\nOne of the ways is to limit downloads into the main dataset while ignoring derivatives/ folder and then establish tuned up crawlers within each derivative.  But that sounds a bit problematic since some tarballs e.g.  https://s3.amazonaws.com/openneuro/ds000030/ds000030_R1.0.4/compressed/ds000030_R1.0.4_metadata_derivatives.zip come with files for both top level and derivatives (mriqc).\r\n\r\nAnother way is to just establish subdatasets, while relying on top level dataset crawler to populate and save them \"together\".  The same strategy would be beneficial for crawling S3 buckets with splitting into subdatasets, where now we establish separate crawlers within subdatasets, thus often \"breaking up the history\" and top dataset history then not reflecting \"versions\" of the entirety of it appropriately. This also would be desired to finally establish HCP datasets (#579) \r\nThe tricky point is that then all dataset/subdatasets would need to be checked out into their corresponding branch (e.g. incoming/ incoming-processed/) and possibly merged (with strategy ours or theirs at times) as a single thing.  So we would need to support such operations as a checkout/merge to operate across hierarchies of the datasets.  Should be quite feasible to implement, and I think it would scale up nicely.\r\n\r\nAnother tricky point would be -- sharing of information about tarballs among (sub)datasets.  E.g. if tarball contains files for multiple (sub)datasets, they all should become aware of the tarball origin.  Ideally, they should even avoid downloading tarball in multiple datasets just to extract their portion.  This will probably be the trickiest point if decided to approach, and I think that last point could be tackled later, first concentrating on making it all work in principle, although possibly incorporating information about where (in dataset hierarchy) tarball came from, could be of help to get it actually handled.  E.g. within dl+archive: url we could add an a new option \"&origds=../\" to point to above dataset which might be installed, and thus the one which might query to get tarball downloaded/extracted and then subdataset only picking the files it needs. Immediate nuance -- clean operation then might need to gain -S option to clean up datasets upwards in the hierarchy. \r\n\r\nMeanwhile I think I will just leave ds000030 at the older version (seems to be the only one with those heavy derivatives such as fmriprep and freesurfer), without introducing derivatives, in hope that we implement this functionality soonish.\r\n@mih and @bpoldrack  - what would be your thoughts on that?", "closed_by": null, "reactions": {"url": "https://api.github.com/repos/datalad/datalad/issues/1671/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/datalad/datalad/issues/1671/timeline", "performed_via_github_app": null}